import{_ as s,c as a,b as p,o as e}from"./app-C01vnHKY.js";const t={};function o(c,n){return e(),a("div",null,n[0]||(n[0]=[p(`<h1 id="答疑篇-思考题答案集锦" tabindex="-1"><a class="header-anchor" href="#答疑篇-思考题答案集锦"><span>答疑篇｜思考题答案集锦</span></a></h1><p>距离我们的专栏更新结束，已经过去不少时间啦。方远老师仍然会在工作之余，回到专栏里转一转，看看同学最新的学习动态。大部分的疑问，老师都在留言区里做了回复。</p><p>除了紧跟更新的第一批同学，也很开心看到有更多新朋友加入到这个专栏的学习中。课程的思考题，为了给你留足思考和研究的时间，我们选择用加餐的方式，把所有参考答案一次性发布出来。</p><p>这里要提醒一下，建议你先自己思考和练习后，再来对答案。每节课都有超链接，方便你跳转回顾。</p><h2 id="第-2-节课" tabindex="-1"><a class="header-anchor" href="#第-2-节课"><span>[第 2 节课]</span></a></h2><p>题目：在刚才用户对游戏评分的那个问题中，你能计算一下每位用户对三款游戏打分的平均分吗？</p><p>答案：</p><div class="language-plain line-numbers-mode" data-highlighter="prismjs" data-ext="plain"><pre><code><span class="line">&gt;&gt;&gt;interest_score.mean(axis=1)</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><h2 id="第-3-节课" tabindex="-1"><a class="header-anchor" href="#第-3-节课"><span>[第 3 节课]</span></a></h2><p>题目：给定数组 scores，形状为（256，256，2），scores[: , :, 0] 与 scores[:, :, 1]对应位置元素的和为 1，现在我们要根据 scores 生产数组 mask，要求 scores 通道 0 的值如果大于通道 1 的值，则 mask 对应的位置为 0，否则为 1。</p><p>scores 如下，你可以试试用代码实现：</p><div class="language-plain line-numbers-mode" data-highlighter="prismjs" data-ext="plain"><pre><code><span class="line">scores = np.random.rand(256, 256, 2)</span>
<span class="line">scores[:,:,1] = 1 - scores[:,:,0]</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div></div></div><p>答案：</p><div class="language-plain line-numbers-mode" data-highlighter="prismjs" data-ext="plain"><pre><code><span class="line">mask = np.argmax(scores, axis=2)</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><h2 id="第-4-节课" tabindex="-1"><a class="header-anchor" href="#第-4-节课"><span>[第 4 节课]</span></a></h2><p>题目：在 PyTorch 中，有 torch.Tensor()和 torch.tensor()两种函数，它们的区别是什么呢？</p><p>答案：torch.Tensor()<strong>是 Pytorch 中的类</strong>，其实它是 torch.FloatTensor()的别名，使用 torch.Tensor()会调用 Tensor 类的构造函数，生成 float 类型的张量；</p><p>而 torch.tensor()<strong>是 Pytorch 的函数</strong>，函数原型是 torch.tensor(data, dtype...)，其中 data 可以是 scalar，list，tuple 等不同的数据结构形式。</p><h2 id="第-5-节课" tabindex="-1"><a class="header-anchor" href="#第-5-节课"><span>[第 5 节课]</span></a></h2><p>题目：现在有个 Tensor，如下。</p><div class="language-python line-numbers-mode" data-highlighter="prismjs" data-ext="py"><pre><code><span class="line"><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> A<span class="token operator">=</span>torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">4</span><span class="token punctuation">,</span><span class="token number">5</span><span class="token punctuation">,</span><span class="token number">7</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">9</span><span class="token punctuation">,</span><span class="token number">8</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span></span>
<span class="line"><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> A</span>
<span class="line">tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">7</span><span class="token punctuation">]</span><span class="token punctuation">,</span></span>
<span class="line">        <span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">9</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">]</span><span class="token punctuation">,</span></span>
<span class="line">        <span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>我们想提取出其中第一行的第一个，第二行的第一第二个，第三行的最后一个，该怎么做呢？</p><p>答案：</p><div class="language-python line-numbers-mode" data-highlighter="prismjs" data-ext="py"><pre><code><span class="line"><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> B<span class="token operator">=</span>torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">type</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>ByteTensor<span class="token punctuation">)</span></span>
<span class="line"><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> B</span>
<span class="line">tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span></span>
<span class="line">        <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span></span>
<span class="line">        <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>uint8<span class="token punctuation">)</span></span>
<span class="line"><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> C<span class="token operator">=</span>torch<span class="token punctuation">.</span>masked_select<span class="token punctuation">(</span>A<span class="token punctuation">,</span>B<span class="token punctuation">)</span></span>
<span class="line"><span class="token operator">&gt;&gt;</span><span class="token operator">&gt;</span> C</span>
<span class="line">tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">9</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">)</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>我们只需要创建一个形状跟 A 一样的 Tensor，然后将对应位置的数值置为 1，然后再把 Tensor 转换成 torch.ByteTensor 类型得到 B，最后跟之前 masked_select 一样的操作就 OK 啦。</p><h2 id="第-6-节课" tabindex="-1"><a class="header-anchor" href="#第-6-节课"><span>[第 6 节课]</span></a></h2><p>题目：在 PyTorch 中，我们要定义一个数据集，应该继承哪一个类呢？</p><p>答案：torch.utils.data.Dataset</p><h2 id="第-7-节课" tabindex="-1"><a class="header-anchor" href="#第-7-节课"><span>[第 7 节课]</span></a></h2><p>题目：Torchvision 中 transforms 模块的作用是什么？</p><p>答案：常用的图像操作，例如随机切割、旋转、Tensor 与 Numpy 和 PIL Image 的数据类型转换等。</p><h2 id="第-8-节课" tabindex="-1"><a class="header-anchor" href="#第-8-节课"><span>[第 8 节课]</span></a></h2><p>题目：请你使用<code>torchvision.models</code>模块实例化一个 VGG 16 网络。</p><p>答案：</p><div class="language-python line-numbers-mode" data-highlighter="prismjs" data-ext="py"><pre><code><span class="line"><span class="token keyword">import</span> torchvision<span class="token punctuation">.</span>models <span class="token keyword">as</span> models</span>
<span class="line">vgg16 <span class="token operator">=</span> models<span class="token punctuation">.</span>vgg16<span class="token punctuation">(</span>pretrained<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div></div></div><h2 id="第-9-节课" tabindex="-1"><a class="header-anchor" href="#第-9-节课"><span>[第 9 节课]</span></a></h2><p>题目：请你想一想，padding 为&#39;same&#39;时，stride 可以为 1 以外的数值吗？</p><p>答案：不可以。</p><h2 id="第-10-节课" tabindex="-1"><a class="header-anchor" href="#第-10-节课"><span>[第 10 节课]</span></a></h2><p>题目：随机生成一个 3 通道的 128x128 的特征图，然后创建一个有 10 个卷积核且卷积核尺寸为 3x3（DW 卷积）的深度可分离卷积，对输入数据进行卷积计算。</p><p>答案：</p><div class="language-plain line-numbers-mode" data-highlighter="prismjs" data-ext="plain"><pre><code><span class="line">import torch</span>
<span class="line">import torch.nn as nn</span>
<span class="line"></span>
<span class="line"># 生成一个三通道的128x128特征图</span>
<span class="line">x = torch.rand((3, 128, 128)).unsqueeze(0)</span>
<span class="line"># DW卷积groups参数与输入通道数一样</span>
<span class="line">dw = nn.Conv2d(x.shape[1], x.shape[1], 3, 1, groups=x.shape[1])</span>
<span class="line">pw = nn.Conv2d(x.shape[1], 10, 1, 1)</span>
<span class="line">out = pw(dw(x))</span>
<span class="line">print(out.shape)</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h2 id="第-11-节课" tabindex="-1"><a class="header-anchor" href="#第-11-节课"><span>[第 11 节课]</span></a></h2><p>题目：损失函数的值越小越好么？</p><p>答案：不是的，咱们在这节课中学习的损失函数，实际上是模型在训练数据上的平均损失，这种损失函数我们称作为经验风险。实际上，还有一个方面也是我们在实际工作中需要考虑的，那就是模型的复杂度：一味追求经验风险的最小化，很容易使得模型过拟合（可回顾一下前文内容）。</p><p>所以，还需要对模型的复杂度进行约束，我们称之为结构风险。实际研发场景中**，最终的损失函数是由经验风险和结构风险共同组成的，我们要求的是两者之和的最小化**。</p><h2 id="第-12-节课" tabindex="-1"><a class="header-anchor" href="#第-12-节课"><span>[第 12 节课]</span></a></h2><p>题目：深度学习都是基于反向传播的么？</p><p>答案：不是的，主流的深度学习模型是基于反向传播和梯度下降的，但是一些非梯度下降的二阶优化算法也是存在的，比如拟牛顿法等。不过计算代价非常大，用的就比较少了。而且一般而言，工业界基本都采用基于反向传播和梯度下降的方式。</p><h2 id="第-13-节课" tabindex="-1"><a class="header-anchor" href="#第-13-节课"><span>[第 13 节课]</span></a></h2><p>题目：batch size 越大越好吗？</p><p>答案：不是的。较大的 batch_size 容易使模型收敛在局部最优点，特别小则容易受噪声影响。</p><h2 id="第-14-节课" tabindex="-1"><a class="header-anchor" href="#第-14-节课"><span>[第 14 节课]</span></a></h2><p>题目：请你自己构建一个卷积神经网络，基于 CIFAR-10，训练一个图像分类模型。因为还没有学习图像分类原理，所以我先帮你写好了网络的结构，需要你补全数据读取、损失函数(交叉熵损失)与优化方法（SGD）等部分。</p><div class="language-python line-numbers-mode" data-highlighter="prismjs" data-ext="py"><pre><code><span class="line"><span class="token keyword">class</span> <span class="token class-name">MyCNN</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span></span>
<span class="line">    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span></span>
<span class="line">        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span></span>
<span class="line">        self<span class="token punctuation">.</span>conv1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">16</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">)</span></span>
<span class="line">        <span class="token comment"># conv1输出的特征图为222x222大小</span></span>
<span class="line">        self<span class="token punctuation">.</span>fc <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">16</span> <span class="token operator">*</span> <span class="token number">222</span> <span class="token operator">*</span> <span class="token number">222</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span></span>
<span class="line"></span>
<span class="line">    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> <span class="token builtin">input</span><span class="token punctuation">)</span><span class="token punctuation">:</span></span>
<span class="line">        x <span class="token operator">=</span> self<span class="token punctuation">.</span>conv1<span class="token punctuation">(</span><span class="token builtin">input</span><span class="token punctuation">)</span></span>
<span class="line">        <span class="token comment"># 进去全连接层之前，先将特征图铺平</span></span>
<span class="line">        x <span class="token operator">=</span> x<span class="token punctuation">.</span>view<span class="token punctuation">(</span>x<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span></span>
<span class="line">        x <span class="token operator">=</span> self<span class="token punctuation">.</span>fc<span class="token punctuation">(</span>x<span class="token punctuation">)</span></span>
<span class="line">        <span class="token keyword">return</span> x</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>答案：</p><div class="language-python line-numbers-mode" data-highlighter="prismjs" data-ext="py"><pre><code><span class="line"><span class="token keyword">import</span> torch</span>
<span class="line"><span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn</span>
<span class="line"><span class="token keyword">import</span> torchvision</span>
<span class="line"><span class="token keyword">import</span> torchvision<span class="token punctuation">.</span>transforms <span class="token keyword">as</span> transforms</span>
<span class="line"><span class="token keyword">from</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data <span class="token keyword">import</span> DataLoader</span>
<span class="line"></span>
<span class="line">transform <span class="token operator">=</span> transforms<span class="token punctuation">.</span>Compose<span class="token punctuation">(</span><span class="token punctuation">[</span></span>
<span class="line">    transforms<span class="token punctuation">.</span>RandomResizedCrop<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">224</span><span class="token punctuation">,</span><span class="token number">224</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span></span>
<span class="line">    transforms<span class="token punctuation">.</span>ToTensor<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span></span>
<span class="line">    transforms<span class="token punctuation">.</span>Normalize<span class="token punctuation">(</span>mean<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">0.485</span><span class="token punctuation">,</span> <span class="token number">0.456</span><span class="token punctuation">,</span> <span class="token number">0.406</span><span class="token punctuation">]</span><span class="token punctuation">,</span> std<span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">0.229</span><span class="token punctuation">,</span> <span class="token number">0.224</span><span class="token punctuation">,</span> <span class="token number">0.225</span><span class="token punctuation">]</span><span class="token punctuation">)</span></span>
<span class="line">    <span class="token punctuation">]</span><span class="token punctuation">)</span></span>
<span class="line">cifar10_dataset <span class="token operator">=</span> torchvision<span class="token punctuation">.</span>datasets<span class="token punctuation">.</span>CIFAR10<span class="token punctuation">(</span>root<span class="token operator">=</span><span class="token string">&#39;./data&#39;</span><span class="token punctuation">,</span></span>
<span class="line">                                       train<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span></span>
<span class="line">                                       transform<span class="token operator">=</span>transform<span class="token punctuation">,</span></span>
<span class="line">                                       target_transform<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span></span>
<span class="line">                                       download<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span></span>
<span class="line">dataloader <span class="token operator">=</span> DataLoader<span class="token punctuation">(</span>dataset<span class="token operator">=</span>cifar10_dataset<span class="token punctuation">,</span> <span class="token comment"># 传入的数据集, 必须参数</span></span>
<span class="line">                               batch_size<span class="token operator">=</span><span class="token number">32</span><span class="token punctuation">,</span>       <span class="token comment"># 输出的batch大小</span></span>
<span class="line">                               shuffle<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span>       <span class="token comment"># 数据是否打乱</span></span>
<span class="line">                               num_workers<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span>      <span class="token comment"># 进程数, 0表示只有主进程</span></span>
<span class="line"></span>
<span class="line"><span class="token keyword">class</span> <span class="token class-name">MyCNN</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span></span>
<span class="line">    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span></span>
<span class="line">        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span></span>
<span class="line">        self<span class="token punctuation">.</span>conv1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">16</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">)</span></span>
<span class="line">        <span class="token comment"># conv1输出的特征图为222x222大小</span></span>
<span class="line">        self<span class="token punctuation">.</span>fc <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">16</span> <span class="token operator">*</span> <span class="token number">222</span> <span class="token operator">*</span> <span class="token number">222</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span></span>
<span class="line"></span>
<span class="line">    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> <span class="token builtin">input</span><span class="token punctuation">)</span><span class="token punctuation">:</span></span>
<span class="line">        x <span class="token operator">=</span> self<span class="token punctuation">.</span>conv1<span class="token punctuation">(</span><span class="token builtin">input</span><span class="token punctuation">)</span></span>
<span class="line">        <span class="token comment"># 进去全连接层之前，先将特征图铺平</span></span>
<span class="line">        x <span class="token operator">=</span> x<span class="token punctuation">.</span>view<span class="token punctuation">(</span>x<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span></span>
<span class="line">        x <span class="token operator">=</span> self<span class="token punctuation">.</span>fc<span class="token punctuation">(</span>x<span class="token punctuation">)</span></span>
<span class="line">        <span class="token keyword">return</span> x</span>
<span class="line">    </span>
<span class="line">cnn <span class="token operator">=</span> MyCNN<span class="token punctuation">(</span><span class="token punctuation">)</span></span>
<span class="line">optimizer <span class="token operator">=</span> torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>SGD<span class="token punctuation">(</span>cnn<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> lr<span class="token operator">=</span><span class="token number">1e-5</span><span class="token punctuation">,</span> weight_decay<span class="token operator">=</span><span class="token number">1e-2</span><span class="token punctuation">,</span> momentum<span class="token operator">=</span><span class="token number">0.9</span><span class="token punctuation">)</span></span>
<span class="line"></span>
<span class="line"><span class="token comment"># 训练3个Epoch</span></span>
<span class="line"><span class="token keyword">for</span> epoch <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">:</span></span>
<span class="line">    <span class="token keyword">for</span> step<span class="token punctuation">,</span> <span class="token punctuation">(</span>images<span class="token punctuation">,</span> target<span class="token punctuation">)</span> <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>dataloader<span class="token punctuation">)</span><span class="token punctuation">:</span></span>
<span class="line">        output <span class="token operator">=</span> cnn<span class="token punctuation">(</span>images<span class="token punctuation">)</span></span>
<span class="line"></span>
<span class="line">        loss <span class="token operator">=</span> nn<span class="token punctuation">.</span>CrossEntropyLoss<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">(</span>output<span class="token punctuation">,</span> target<span class="token punctuation">)</span></span>
<span class="line">        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&#39;Epoch: {} Step: {} Loss: {}&#39;</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>epoch <span class="token operator">+</span> <span class="token number">1</span> <span class="token punctuation">,</span> step<span class="token punctuation">,</span> loss<span class="token punctuation">)</span><span class="token punctuation">)</span></span>
<span class="line">        cnn<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span></span>
<span class="line">        loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span></span>
<span class="line">        optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h2 id="第-15-节课" tabindex="-1"><a class="header-anchor" href="#第-15-节课"><span>[第 15 节课]</span></a></h2><p>题目：参考 Visdom 快速上手中的例子，现在需要生成两组随机数，分别表示 Loss 和 Accuracy。在迭代的过程中，如何用代码同时绘制出 Loss 和 Accuracy 两组数据呢？</p><p>答案：</p><div class="language-python line-numbers-mode" data-highlighter="prismjs" data-ext="py"><pre><code><span class="line"><span class="token keyword">from</span> visdom <span class="token keyword">import</span> Visdom</span>
<span class="line"><span class="token keyword">import</span> numpy <span class="token keyword">as</span> np</span>
<span class="line"><span class="token keyword">import</span> time</span>
<span class="line"></span>
<span class="line"><span class="token comment"># 实例化窗口</span></span>
<span class="line">viz <span class="token operator">=</span> Visdom<span class="token punctuation">(</span>port<span class="token operator">=</span><span class="token number">6006</span><span class="token punctuation">)</span> </span>
<span class="line"><span class="token comment"># 初始化窗口参数</span></span>
<span class="line">viz<span class="token punctuation">.</span>line<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">0.</span><span class="token punctuation">,</span><span class="token number">0.</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">0.</span><span class="token punctuation">]</span><span class="token punctuation">,</span></span>
<span class="line">         win<span class="token operator">=</span><span class="token string">&#39;train&#39;</span><span class="token punctuation">,</span></span>
<span class="line">         opts<span class="token operator">=</span><span class="token builtin">dict</span><span class="token punctuation">(</span>title<span class="token operator">=</span><span class="token string">&#39;loss&amp;acc&#39;</span><span class="token punctuation">,</span> legend<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">&#39;loss&#39;</span><span class="token punctuation">,</span><span class="token string">&#39;acc&#39;</span><span class="token punctuation">]</span><span class="token punctuation">)</span></span>
<span class="line">         <span class="token punctuation">)</span></span>
<span class="line"></span>
<span class="line"><span class="token keyword">for</span> step <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">:</span></span>
<span class="line">    loss <span class="token operator">=</span> <span class="token number">0.2</span> <span class="token operator">*</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token number">1</span></span>
<span class="line">    acc <span class="token operator">=</span> <span class="token number">0.1</span> <span class="token operator">*</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">+</span> <span class="token number">0.5</span></span>
<span class="line">    <span class="token comment"># 更新窗口数据</span></span>
<span class="line">    viz<span class="token punctuation">.</span>line<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span>loss<span class="token punctuation">,</span> acc<span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span>step<span class="token punctuation">]</span><span class="token punctuation">,</span> win<span class="token operator">=</span><span class="token string">&#39;train&#39;</span><span class="token punctuation">,</span> update<span class="token operator">=</span><span class="token string">&#39;append&#39;</span><span class="token punctuation">)</span></span>
<span class="line">    time<span class="token punctuation">.</span>sleep<span class="token punctuation">(</span><span class="token number">0.5</span><span class="token punctuation">)</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>运行结果如图所示：</p><p><img src="https://static001.geekbang.org/resource/image/7e/18/7eecfc47a74d6fa319d45eb6092dec18.jpg?wh=1920x1075" alt="图片"></p><h2 id="第-16-节课" tabindex="-1"><a class="header-anchor" href="#第-16-节课"><span>[第 16 节课]</span></a></h2><p>题目：在 torch.distributed.init_process_group(backend=&quot;nccl&quot;)函数中，backend 参数可选哪些后端，它们分别有什么区别？</p><p>答案：backend 参数指定的通信后端，包括 NCCL、MPI、gloo。NCCL 是 Nvidia 提供的官方多卡通信框架，相对比较高效；MPI 也是高性能计算常用的通信协议，但是需要自己安装 MPI 实现框架，例如 OpenMPI；gloo 是内置通信后端，但不够高效。</p><h2 id="第-18-节课" tabindex="-1"><a class="header-anchor" href="#第-18-节课"><span>[第 18 节课]</span></a></h2><p>题目：老板希望你的模型能尽可能的把线上所有极客时间的海报都找到，允许一些误召回。训练模型的时候你应该侧重精确率还是召回率？</p><p>答案：侧重召回率。</p><h2 id="第-19-节课" tabindex="-1"><a class="header-anchor" href="#第-19-节课"><span>[第 19 节课]</span></a></h2><p>题目：对于这节课里讲的小猫分割问题，最终只输出 1 个特征图是否可以？</p><p>答案：可以的，因为小猫分割是一个二分类问题，可以将输出的特征图使用 sigmoid 函数将输出的数值转换为一个概率，从而进行判断。</p><h2 id="第-20-节课" tabindex="-1"><a class="header-anchor" href="#第-20-节课"><span>[第 20 节课]</span></a></h2><p>题目：图像分割的评价指标都有什么？</p><p>答案：mIoU 和 Dice 系数。</p><h2 id="第-21-节课" tabindex="-1"><a class="header-anchor" href="#第-21-节课"><span>[第 21 节课]</span></a></h2><p>题目：TF-IDF 有哪些缺点呢？你不妨结合它的计算过程做个梳理。</p><p>答案：TF-IDF 认为文本频率小的单词就越重要，也就是区分性越强，但是实际上很多情况下，这并不正确。比如一篇财经类文章有一句“股价就跟火箭一样上了天”，这里的“火箭”就会变得非常重要，显然是错误的。怎么办呢？一般我们会对词频做一个条件过滤，比如超过多少次。也会对 TF-IDF 的公式进行改进，具体改进方法，如果有兴趣的话，你可以借助网络查找相应的文章。</p><h2 id="第-22-节课" tabindex="-1"><a class="header-anchor" href="#第-22-节课"><span>[第 22 节课]</span></a></h2><p>题目：词向量的长度多少比较合适呢？越长越好吗？</p><p>答案：不是的，越长的词向量尽管可以更加精细的表示词语的空间位置，但是也会带来计算量的暴涨、数据稀疏等问题，一般来说我们较多的选择 64、128、256 这样的长度，具体是多少，要靠实验来不断的确定。有的论文给出的建议是 n&gt;8.33logN，具体是否可行，还是要结合实际情况来敲定。</p><h2 id="第-23-节课" tabindex="-1"><a class="header-anchor" href="#第-23-节课"><span>[第 23 节课]</span></a></h2><p>题目：利用今天训练的模型，编写一个函数 predict_sentiment，实现输入一句话，输出这句话的情绪类别与概率。</p><p>例如：</p><div class="language-python line-numbers-mode" data-highlighter="prismjs" data-ext="py"><pre><code><span class="line">text <span class="token operator">=</span> <span class="token string">&quot;This film is terrible!&quot;</span></span>
<span class="line">predict_sentiment<span class="token punctuation">(</span>text<span class="token punctuation">,</span> model<span class="token punctuation">,</span> tokenizer<span class="token punctuation">,</span> vocab<span class="token punctuation">,</span> device<span class="token punctuation">)</span></span>
<span class="line"><span class="token triple-quoted-string string">&#39;&#39;&#39;</span>
<span class="line">输出：(&#39;neg&#39;, 0.8874172568321228)</span>
<span class="line">&#39;&#39;&#39;</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>答案：参考代码如下。</p><div class="language-python line-numbers-mode" data-highlighter="prismjs" data-ext="py"><pre><code><span class="line"><span class="token comment"># 预测过程</span></span>
<span class="line"><span class="token keyword">def</span> <span class="token function">predict_sentiment</span><span class="token punctuation">(</span>text<span class="token punctuation">,</span> model<span class="token punctuation">,</span> tokenizer<span class="token punctuation">,</span> vocab<span class="token punctuation">,</span> device<span class="token punctuation">)</span><span class="token punctuation">:</span></span>
<span class="line">    tokens <span class="token operator">=</span> tokenizer<span class="token punctuation">(</span>text<span class="token punctuation">)</span></span>
<span class="line">    ids <span class="token operator">=</span> <span class="token punctuation">[</span>vocab<span class="token punctuation">[</span>t<span class="token punctuation">]</span> <span class="token keyword">for</span> t <span class="token keyword">in</span> tokens<span class="token punctuation">]</span></span>
<span class="line">    length <span class="token operator">=</span> torch<span class="token punctuation">.</span>LongTensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token builtin">len</span><span class="token punctuation">(</span>ids<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span></span>
<span class="line">    tensor <span class="token operator">=</span> torch<span class="token punctuation">.</span>LongTensor<span class="token punctuation">(</span>ids<span class="token punctuation">)</span><span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span></span>
<span class="line">    prediction <span class="token operator">=</span> model<span class="token punctuation">(</span>tensor<span class="token punctuation">,</span> length<span class="token punctuation">)</span><span class="token punctuation">.</span>squeeze<span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span></span>
<span class="line">    probability <span class="token operator">=</span> torch<span class="token punctuation">.</span>softmax<span class="token punctuation">(</span>prediction<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span></span>
<span class="line">    predicted_class <span class="token operator">=</span> prediction<span class="token punctuation">.</span>argmax<span class="token punctuation">(</span>dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span></span>
<span class="line">    predicted_probability <span class="token operator">=</span> probability<span class="token punctuation">[</span>predicted_class<span class="token punctuation">]</span><span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span></span>
<span class="line">    predicted_class <span class="token operator">=</span> <span class="token string">&#39;neg&#39;</span> <span class="token keyword">if</span> predicted_class <span class="token operator">==</span> <span class="token number">0</span> <span class="token keyword">else</span> <span class="token string">&#39;pos&#39;</span></span>
<span class="line">    <span class="token keyword">return</span> predicted_class<span class="token punctuation">,</span> predicted_probability</span>
<span class="line"></span>
<span class="line"><span class="token comment"># 加载模型</span></span>
<span class="line">model<span class="token punctuation">.</span>load_state_dict<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>load<span class="token punctuation">(</span><span class="token string">&#39;lstm.pt&#39;</span><span class="token punctuation">)</span><span class="token punctuation">)</span></span>
<span class="line"></span>
<span class="line">text <span class="token operator">=</span> <span class="token string">&quot;This film is terrible!&quot;</span></span>
<span class="line">predict_sentiment<span class="token punctuation">(</span>text<span class="token punctuation">,</span> model<span class="token punctuation">,</span> tokenizer<span class="token punctuation">,</span> vocab<span class="token punctuation">,</span> device<span class="token punctuation">)</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h2 id="第-24-节课" tabindex="-1"><a class="header-anchor" href="#第-24-节课"><span>[第 24 节课]</span></a></h2><p>题目：Bert 处理文本是有最大长度要求的（512），那么遇到长文本，该怎么办呢？</p><p>答案：这是一个非常开放的问题，设置为最大 512 主要还是兼顾了效率问题，但还是有非常多的解决办法，比如我们之前提到过的关键词提取。或者分别从开头、中间、结尾选择一定长度的内容做运算。不过这些都是比较简单的办法。你还有更好的办法吗？欢迎留言给我。</p><h2 id="第-25-节课" tabindex="-1"><a class="header-anchor" href="#第-25-节课"><span>[第 25 节课]</span></a></h2><p>题目：自 2018 年 BERT 被提出以来，获得了很大的成功，学术界陆续提出了各类相关模型，例如我们今天学习的 BART。请你查一查还有哪些 BERT 系列的模型，并阅读相关论文，自行学习一下它们的原理与特点。</p><p>答案：</p><p><a href="https://arxiv.org/abs/1906.08237" target="_blank" rel="noopener noreferrer">XLNet: Generalized Autoregressive Pretraining for Language Understanding</a></p><p><a href="https://arxiv.org/abs/1907.11692" target="_blank" rel="noopener noreferrer">RoBERTa: A Robustly Optimized BERT Pretraining Approach</a></p><p><a href="https://arxiv.org/abs/1909.11942" target="_blank" rel="noopener noreferrer">ALBERT: A Lite BERT for Self-supervised Learning of Language Representations</a></p><p>最后，再次祝愿你虎年快乐，学习进步，工作顺利！</p>`,97)]))}const i=s(t,[["render",o]]),u=JSON.parse('{"path":"/3.tech/83.PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98/29_%E7%AD%94%E7%96%91%E7%AF%87%EF%BD%9C%E6%80%9D%E8%80%83%E9%A2%98%E7%AD%94%E6%A1%88%E9%9B%86%E9%94%A6.html","title":"答疑篇｜思考题答案集锦","lang":"zh-cn","frontmatter":{},"headers":[{"level":2,"title":"[第 2 节课]","slug":"第-2-节课","link":"#第-2-节课","children":[]},{"level":2,"title":"[第 3 节课]","slug":"第-3-节课","link":"#第-3-节课","children":[]},{"level":2,"title":"[第 4 节课]","slug":"第-4-节课","link":"#第-4-节课","children":[]},{"level":2,"title":"[第 5 节课]","slug":"第-5-节课","link":"#第-5-节课","children":[]},{"level":2,"title":"[第 6 节课]","slug":"第-6-节课","link":"#第-6-节课","children":[]},{"level":2,"title":"[第 7 节课]","slug":"第-7-节课","link":"#第-7-节课","children":[]},{"level":2,"title":"[第 8 节课]","slug":"第-8-节课","link":"#第-8-节课","children":[]},{"level":2,"title":"[第 9 节课]","slug":"第-9-节课","link":"#第-9-节课","children":[]},{"level":2,"title":"[第 10 节课]","slug":"第-10-节课","link":"#第-10-节课","children":[]},{"level":2,"title":"[第 11 节课]","slug":"第-11-节课","link":"#第-11-节课","children":[]},{"level":2,"title":"[第 12 节课]","slug":"第-12-节课","link":"#第-12-节课","children":[]},{"level":2,"title":"[第 13 节课]","slug":"第-13-节课","link":"#第-13-节课","children":[]},{"level":2,"title":"[第 14 节课]","slug":"第-14-节课","link":"#第-14-节课","children":[]},{"level":2,"title":"[第 15 节课]","slug":"第-15-节课","link":"#第-15-节课","children":[]},{"level":2,"title":"[第 16 节课]","slug":"第-16-节课","link":"#第-16-节课","children":[]},{"level":2,"title":"[第 18 节课]","slug":"第-18-节课","link":"#第-18-节课","children":[]},{"level":2,"title":"[第 19 节课]","slug":"第-19-节课","link":"#第-19-节课","children":[]},{"level":2,"title":"[第 20 节课]","slug":"第-20-节课","link":"#第-20-节课","children":[]},{"level":2,"title":"[第 21 节课]","slug":"第-21-节课","link":"#第-21-节课","children":[]},{"level":2,"title":"[第 22 节课]","slug":"第-22-节课","link":"#第-22-节课","children":[]},{"level":2,"title":"[第 23 节课]","slug":"第-23-节课","link":"#第-23-节课","children":[]},{"level":2,"title":"[第 24 节课]","slug":"第-24-节课","link":"#第-24-节课","children":[]},{"level":2,"title":"[第 25 节课]","slug":"第-25-节课","link":"#第-25-节课","children":[]}],"git":{"updatedTime":1746672966000,"contributors":[{"name":"guoxin-qiu","username":"guoxin-qiu","email":"guoxin.qiu@outlook.com","commits":2,"url":"https://github.com/guoxin-qiu"}],"changelog":[{"hash":"873191059aa4709eddd6184a409223b5054edb2a","time":1746672966000,"email":"guoxin.qiu@outlook.com","author":"guoxin-qiu","message":"update: pytorch fixed"},{"hash":"b44b80ec6b8c2ebffa55c7b2b54259609c76baed","time":1745668690000,"email":"guoxin.qiu@outlook.com","author":"guoxin-qiu","message":"add pytorch course"}]},"filePathRelative":"3.tech/83.PyTorch深度学习实战/29_答疑篇｜思考题答案集锦.md"}');export{i as comp,u as data};
