import{_ as s,c as a,a as e,o as t}from"./app-Dmwo-0Oh.js";const p={};function i(l,n){return t(),a("div",null,n[0]||(n[0]=[e(`<h1 id="实战篇-摘要-如何快速实现自动文摘生成" tabindex="-1"><a class="header-anchor" href="#实战篇-摘要-如何快速实现自动文摘生成"><span>实战篇-摘要：如何快速实现自动文摘生成</span></a></h1><p>当我们打开某个新闻 APP 或者某个网站时，常常被这样的标题所吸引：“震惊了十亿人”、“一定要读完，跟你的生命有关！”等。但是当我们点进去却发现都是标题党，实际内容大相径庭！这时候你可能会想，如果有一种工具能帮助我们提炼文章的关键内容，那我们就不会再受到标题党的影响了。其实想要实现这个工具并不复杂，用自动文摘技术就能解决。</p><p>自动文摘充斥着我们生活的方方面面，它可用于热点新闻聚合、新闻推荐、语音播报、APP 消息 Push、智能写作等场景。今天我们要讲的这个自然语言处理任务，就是自动文摘生成。</p><h2 id="问题背景" tabindex="-1"><a class="header-anchor" href="#问题背景"><span>问题背景</span></a></h2><p>自动文摘技术，就是自动提炼出一些句子来概括整篇文章的大意，用户通过读摘要就可以了解到原文要表达的意思。</p><h3 id="抽取与生成" tabindex="-1"><a class="header-anchor" href="#抽取与生成"><span>抽取与生成</span></a></h3><p>自动文摘有两种解决方案：一种是抽取式（Extractive）的，就是从原文中提取一些关键的句子，组合成一篇摘要；另外一种是生成式（Abstractive）的，也是这节课我们重点要讲的内容，这种方式需要计算机通读原文后，在理解整篇文章内容的基础上，使用简短连贯的语言将原文的主要内容表达出来，即会产生原文中没有出现的词和句子。</p><p>现阶段，抽取式的摘要目前已经相对成熟，但是抽取质量及内容流畅度都不够理想。随着深度学习的研究，生成式摘要的质量和流畅度都有很大提升，但目前也受到原文本长度过长、抽取内容不佳等限制，生成的摘要与人工摘要相比，还有相当的差距。</p><p>语言的表达方式多种多样，机器生成的摘要可能和人工摘要并不相同，那么如何衡量自动摘要的好坏呢？这就涉及到摘要的评价指标。</p><h3 id="评价指标" tabindex="-1"><a class="header-anchor" href="#评价指标"><span>评价指标</span></a></h3><p>评价自动摘要的效果通常使用 <strong>ROUGE</strong>（Recall Oriented Understudy for Gisting Evaluation）评价。</p><p>ROUGE 评价法参考了机器翻译自动评价方法，并且考虑了 N-gram 共同出现的程度。这个方法具体是这样设计的：首先由多个专家分别生成人工摘要，构成标准摘要集；然后对比系统生成的自动摘要与人工生成的标准摘要，通过统计二者之间重叠的基本单元（n 元语法、词序或词对）的数目，来评价摘要的质量。通过与多专家人工摘要的对比，提高评价系统的稳定性和健壮性。</p><p>ROUGE 主要包括以下 4 种评价指标：</p><p>1.ROUGE-N，基于 n-gram 的共现统计； 2.ROUGE-L，基于最长公共子串； 3.ROUGE-S，基于顺序词对统计； 4.ROUGE-W，在 ROUGE-L 的基础上，考虑串的连续匹配。  了解了自动文摘的种类与评价指标，下面我们再来认识一个用于自动文摘生成的模型——BART。它的名字和上节课讲过的 BERT 非常像，我们先来看看它有哪些特点。</p><h2 id="bart-原理与特点分析" tabindex="-1"><a class="header-anchor" href="#bart-原理与特点分析"><span>BART 原理与特点分析</span></a></h2><p>BART 的全称是 Bidirectional and Auto-Regressive Transformers（双向自回归变压器）。它是由 Facebook AI 在 2019 年提出的一个新的预训练模型，结合了双向 Transformer 和自回归 Transformer，在文本生成相关任务中达到了 SOTA 的结果。你可以通过这个链接查看<a href="https://arxiv.org/abs/1910.13461" target="_blank" rel="noopener noreferrer">相关论文</a>。</p><p>我们已经熟知了论文《Attention is all you need》中提出的 Transformer。Transformer 左半边为 Encoder，右半边为 Decoder。Encoder 和 Decoder 的结构分别如下图（a）、（b）所示。</p><p><img src="https://static001.geekbang.org/resource/image/02/c7/02d7541cd7b6f0a8b8c35efb3e4d74c7.jpg?wh=1920x901" alt="图片" title="图片来源：https://arxiv.org/abs/1910.13461"></p><p>Encoder 负责将原始文本进行 self-attention，并获得句子中每个词的词向量，最经典的 Encoder 架构就是上节课所学习的 BERT，但是<strong>单独 Encoder 结构不适用于文本生成任务</strong>。</p><p>Decoder 的输入与输出之间错开一个位置，这是为了模拟文本生成时，不能让模型看到未来的词，这种方式称为 Auto-Regressive（自回归）。例如 GPT 等<strong>基于 Decoder 结构</strong>的模型通常适用于做文本生成任务，但是<strong>无法学习双向的上下文语境信息</strong>。</p><p>BART 模型就是将 Encoder 和 Decoder 结合在一起的一种 sequence-to-sequence 结构，它的主要结构如下图所示。</p><p><img src="https://static001.geekbang.org/resource/image/a6/99/a663e08e28803d6059aae93fea1a0699.png?wh=1898x778" alt="图片" title="图片来源：https://arxiv.org/abs/1910.13461"></p><p>BART 模型的结构看似与 Transformer 没什么不同，主要区别在于 BART 的预训练阶段。首先在 Encoder 端使用多种噪声对原始文本进行<strong>破坏</strong>，然后再使用 Decoder <strong>重建</strong>原始文本。</p><p>由于 BART 本身就是在 sequence-to-sequence 的基础上构建并且进行预训练，它天然就适合做序列生成的任务，例如：问答、文本摘要、机器翻译等。在生成任务上获得进步的同时，在一些文本理解类任务上它也可以取得很好的效果。</p><p>下面我们进入实战阶段，利用 BART 来实现自动文摘生成。</p><h2 id="快速文摘生成" tabindex="-1"><a class="header-anchor" href="#快速文摘生成"><span>快速文摘生成</span></a></h2><p>这里我们还是使用 hugging face 的 Transformers 工具包。具体的安装过程，上一节课已经介绍过了。</p><p>Transformers 工具包为快速使用自动文摘生成模型提供了 pipeline API。pipeline 聚合了文本预处理步骤与训练好的自动文摘生成模型。利用 Transformers 的 pipeline，我们只需短短几行代码，就可以快速生成文本摘要。</p><p>下面是一个使用 pipeline 生成文摘的例子，代码如下。</p><div class="language-python line-numbers-mode" data-highlighter="prismjs" data-ext="py"><pre><code><span class="line"><span class="token keyword">from</span> transformers <span class="token keyword">import</span> pipeline</span>
<span class="line"></span>
<span class="line">summarizer <span class="token operator">=</span> pipeline<span class="token punctuation">(</span><span class="token string">&quot;summarization&quot;</span><span class="token punctuation">)</span></span>
<span class="line"></span>
<span class="line">ARTICLE <span class="token operator">=</span> <span class="token triple-quoted-string string">&quot;&quot;&quot; New York (CNN)When Liana Barrientos was 23 years old, she got married in Westchester County, New York.</span>
<span class="line">A year later, she got married again in Westchester County, but to a different man and without divorcing her first husband.</span>
<span class="line">Only 18 days after that marriage, she got hitched yet again. Then, Barrientos declared &quot;I do&quot; five more times, sometimes only within two weeks of each other.</span>
<span class="line">In 2010, she married once more, this time in the Bronx. In an application for a marriage license, she stated it was her &quot;first and only&quot; marriage.</span>
<span class="line">Barrientos, now 39, is facing two criminal counts of &quot;offering a false instrument for filing in the first degree,&quot; referring to her false statements on the</span>
<span class="line">2010 marriage license application, according to court documents.</span>
<span class="line">Prosecutors said the marriages were part of an immigration scam.</span>
<span class="line">On Friday, she pleaded not guilty at State Supreme Court in the Bronx, according to her attorney, Christopher Wright, who declined to comment further.</span>
<span class="line">After leaving court, Barrientos was arrested and charged with theft of service and criminal trespass for allegedly sneaking into the New York subway through an emergency exit, said Detective</span>
<span class="line">Annette Markowski, a police spokeswoman. In total, Barrientos has been married 10 times, with nine of her marriages occurring between 1999 and 2002.</span>
<span class="line">All occurred either in Westchester County, Long Island, New Jersey or the Bronx. She is believed to still be married to four men, and at one time, she was married to eight men at once, prosecutors say.</span>
<span class="line">Prosecutors said the immigration scam involved some of her husbands, who filed for permanent residence status shortly after the marriages.</span>
<span class="line">Any divorces happened only after such filings were approved. It was unclear whether any of the men will be prosecuted.</span>
<span class="line">The case was referred to the Bronx District Attorney\\&#39;s Office by Immigration and Customs Enforcement and the Department of Homeland Security\\&#39;s</span>
<span class="line">Investigation Division. Seven of the men are from so-called &quot;red-flagged&quot; countries, including Egypt, Turkey, Georgia, Pakistan and Mali.</span>
<span class="line">Her eighth husband, Rashid Rajput, was deported in 2006 to his native Pakistan after an investigation by the Joint Terrorism Task Force.</span>
<span class="line">If convicted, Barrientos faces up to four years in prison.  Her next court appearance is scheduled for May 18.</span>
<span class="line">&quot;&quot;&quot;</span></span>
<span class="line"></span>
<span class="line"><span class="token keyword">print</span><span class="token punctuation">(</span>summarizer<span class="token punctuation">(</span>ARTICLE<span class="token punctuation">,</span> max_length<span class="token operator">=</span><span class="token number">130</span><span class="token punctuation">,</span> min_length<span class="token operator">=</span><span class="token number">30</span><span class="token punctuation">)</span><span class="token punctuation">)</span></span>
<span class="line"><span class="token triple-quoted-string string">&#39;&#39;&#39;</span>
<span class="line">输出:</span>
<span class="line">[{&#39;summary_text&#39;: &#39; Liana Barrientos, 39, is charged with two counts of &quot;offering a false instrument for filing in</span>
<span class="line">the first degree&quot; In total, she has been married 10 times, with nine of her marriages occurring between 1999 and</span>
<span class="line">2002 . At one time, she was married to eight men at once, prosecutors say .&#39;}]</span>
<span class="line">&#39;&#39;&#39;</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>第 3 行代码的作用是构建一个自动文摘的 pipeline，pipeline 会自动下载并缓存训练好的自动文摘生成模型。这个自动文摘生成模型是 BART 模型在 CNN/Daily Mail 数据集上训练得到的。</p><p>第 5~22 行代码是待生成摘要的文章原文。第 24 行代码是针对文摘原文自动生成文摘，其中参数 max_length 和 min_length 限制了文摘的最大和最小长度，输出的结果如上面代码注释所示。</p><p>如果你不想使用 Transformers 提供的预训练模型，而是想使用自己的模型或其它任意模型也很简单。具体代码如下。</p><div class="language-python line-numbers-mode" data-highlighter="prismjs" data-ext="py"><pre><code><span class="line"><span class="token keyword">from</span> transformers <span class="token keyword">import</span> BartTokenizer<span class="token punctuation">,</span> BartForConditionalGeneration</span>
<span class="line"></span>
<span class="line">model <span class="token operator">=</span> BartForConditionalGeneration<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span><span class="token string">&#39;facebook/bart-large-cnn&#39;</span><span class="token punctuation">)</span></span>
<span class="line">tokenizer <span class="token operator">=</span> BartTokenizer<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span><span class="token string">&#39;facebook/bart-large-cnn&#39;</span><span class="token punctuation">)</span></span>
<span class="line"></span>
<span class="line">inputs <span class="token operator">=</span> tokenizer<span class="token punctuation">(</span><span class="token punctuation">[</span>ARTICLE<span class="token punctuation">]</span><span class="token punctuation">,</span> max_length<span class="token operator">=</span><span class="token number">1024</span><span class="token punctuation">,</span> return_tensors<span class="token operator">=</span><span class="token string">&#39;pt&#39;</span><span class="token punctuation">)</span></span>
<span class="line"></span>
<span class="line"><span class="token comment"># 生成文摘</span></span>
<span class="line">summary_ids <span class="token operator">=</span> model<span class="token punctuation">.</span>generate<span class="token punctuation">(</span>inputs<span class="token punctuation">[</span><span class="token string">&#39;input_ids&#39;</span><span class="token punctuation">]</span><span class="token punctuation">,</span> max_length<span class="token operator">=</span><span class="token number">130</span><span class="token punctuation">,</span> early_stopping<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span></span>
<span class="line">summary <span class="token operator">=</span> tokenizer<span class="token punctuation">.</span>decode<span class="token punctuation">(</span>summary_ids<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> skip_special_tokens<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span></span>
<span class="line"><span class="token keyword">print</span><span class="token punctuation">(</span>summary<span class="token punctuation">)</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>流程是一共包括四步，我们分别看一下。 第一步是实例化一个 BART 的模型和分词器对象。BartForConditionalGeneration 类是 BART 模型用于摘要生成的类，BartTokenizer 是 BART 的分词器，它们都有 from_pretrained()方法，可以加载预训练模型。</p><p>from_pretrained()函数需要传入一个字符串作为参数，这个字符串可以是本地模型的路径，也可以是上传到 Hugging Face 模型库中的模型名字。</p><p>这里“facebook/bart-large-cnn”是 Facebook 利用 CNN/Daily Mail 数据集训练的 BART 模型，模型具体细节你可以参考<a href="https://huggingface.co/facebook/bart-large-cnn" target="_blank" rel="noopener noreferrer">这里</a>。</p><p>接下来是第二步，对原始文本进行分词。我们可以利用分词器对象 tokenizer 对原始文本 ARTICLE 进行分词，并得到词语 id 的 Tensor。return_tensors=&#39;pt&#39;表示返回值是 PyTorch 的 Tensor。</p><p>第三步，使用 generate()方法生成摘要。其中参数 max_length 限制了生成摘要的最大长度，early_stopping 表示生成过程是否可提前停止。generate()方法的输出是摘要词语的 id。</p><p>最后一步，利用分词器解码得到最终的摘要文本。利用 tokenizer.decode()函数，将词语 id 转换为词语文本。其中参数 skip_special_tokens 表示是否去掉“pad”一些特殊 token。</p><h2 id="fine-tuning-bart" tabindex="-1"><a class="header-anchor" href="#fine-tuning-bart"><span>Fine-tuning BART</span></a></h2><p>下面我们来看一看如何用自己的数据集来训练 BART 模型。</p><p>​</p><h3 id="模型加载" tabindex="-1"><a class="header-anchor" href="#模型加载"><span>模型加载</span></a></h3><p>模型加载部分和之前讲的一样，不再过多重复。这里我们要利用 BartForConditionalGeneration 类的 from_pretrained()函数，加载一个 BART 模型。</p><p>模型加载的代码如下。这里我们会在 Facebook 训练好的摘要模型上，继续 Fine-tuning。</p><div class="language-python line-numbers-mode" data-highlighter="prismjs" data-ext="py"><pre><code><span class="line"><span class="token keyword">from</span> transformers <span class="token keyword">import</span> BartTokenizer<span class="token punctuation">,</span> BartForConditionalGeneration</span>
<span class="line"></span>
<span class="line">tokenizer <span class="token operator">=</span> BartTokenizer<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span><span class="token string">&#39;facebook/bart-large-cnn&#39;</span><span class="token punctuation">)</span></span>
<span class="line">model <span class="token operator">=</span> BartForConditionalGeneration<span class="token punctuation">.</span>from_pretrained<span class="token punctuation">(</span><span class="token string">&#39;facebook/facebook/bart-large-cnn&#39;</span><span class="token punctuation">)</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h3 id="数据准备" tabindex="-1"><a class="header-anchor" href="#数据准备"><span>数据准备</span></a></h3><p>接下来，是数据准备。我们先来回顾一下，之前学习过的读取文本数据集的方式。在[第 6 课]中，我们学习过使用 PyTorch 原生的的 Dataset 类读取数据集；在[第 23 课]中，我们学习了使用 Torchtext 工具<code>torchtext.datasets</code>来读取数据集。今天，我们还要学习一种新的数据读取工具：Datasets 库。</p><p>Datasets 库也是由 hugging face 团队开发的，旨在轻松访问与共享数据集。官方的文档在<a href="https://huggingface.co/docs/datasets/index.html" target="_blank" rel="noopener noreferrer">这里</a>，有兴趣了解更多的同学可以去看看。</p><p>Datasets 库的安装同样非常简单。可以使用 pip 安装：</p><div class="language-python line-numbers-mode" data-highlighter="prismjs" data-ext="py"><pre><code><span class="line">pip install datasets</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><p>或使用 conda 进行安装：</p><div class="language-python line-numbers-mode" data-highlighter="prismjs" data-ext="py"><pre><code><span class="line">conda install <span class="token operator">-</span>c huggingface <span class="token operator">-</span>c conda<span class="token operator">-</span>forge datasets</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><p>Datasets 库中同样包括常见数据集，而且帮我们封装好了读取数据集的操作。我们来看一个读取 IMDB 数据集（第 23 课讲过）的训练数据的示例：</p><div class="language-python line-numbers-mode" data-highlighter="prismjs" data-ext="py"><pre><code><span class="line"><span class="token keyword">import</span> datasets</span>
<span class="line">train_dataset <span class="token operator">=</span> datasets<span class="token punctuation">.</span>load_dataset<span class="token punctuation">(</span><span class="token string">&quot;imdb&quot;</span><span class="token punctuation">,</span> split<span class="token operator">=</span><span class="token string">&quot;train&quot;</span><span class="token punctuation">)</span></span>
<span class="line"><span class="token keyword">print</span><span class="token punctuation">(</span>train_dataset<span class="token punctuation">.</span>column_names<span class="token punctuation">)</span></span>
<span class="line"><span class="token triple-quoted-string string">&#39;&#39;&#39;</span>
<span class="line">输出：</span>
<span class="line">[&#39;label&#39;, &#39;text&#39;]</span>
<span class="line">&#39;&#39;&#39;</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>用 load_dataset()函数来加载数据集，它的参数是数据集的名字或本地文件的路径，split 参数用于指定加载训练集、测试集或验证集。</p><p>我们还可以从不止一个 csv 文件中加载数据：</p><div class="language-python line-numbers-mode" data-highlighter="prismjs" data-ext="py"><pre><code><span class="line">data_files <span class="token operator">=</span> <span class="token punctuation">{</span><span class="token string">&quot;train&quot;</span><span class="token punctuation">:</span> <span class="token string">&quot;train.csv&quot;</span><span class="token punctuation">,</span> <span class="token string">&quot;test&quot;</span><span class="token punctuation">:</span> <span class="token string">&quot;test.csv&quot;</span><span class="token punctuation">}</span></span>
<span class="line">dataset <span class="token operator">=</span> load_dataset<span class="token punctuation">(</span><span class="token string">&quot;namespace/your_dataset_name&quot;</span><span class="token punctuation">,</span> data_files<span class="token operator">=</span>data_files<span class="token punctuation">)</span></span>
<span class="line"><span class="token keyword">print</span><span class="token punctuation">(</span>datasets<span class="token punctuation">)</span></span>
<span class="line"><span class="token triple-quoted-string string">&#39;&#39;&#39;</span>
<span class="line">示例输出：(实际输出与此不同)</span>
<span class="line">{train: Dataset({</span>
<span class="line">    features: [&#39;idx&#39;, &#39;text&#39;, &#39;summary&#39;],</span>
<span class="line">    num_rows: 3668</span>
<span class="line">})</span>
<span class="line">test: Dataset({</span>
<span class="line">    features: [&#39;idx&#39;, &#39;text&#39;, &#39;summary&#39;],</span>
<span class="line">    num_rows: 1725</span>
<span class="line">})</span>
<span class="line">}</span>
<span class="line">&#39;&#39;&#39;</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>通过参数 data_files 指定训练集、测试集或验证集所需加载的文件路径即可。 我们可以使用 map()函数来对数据集进行一些预处理操作，示例如下：</p><div class="language-python line-numbers-mode" data-highlighter="prismjs" data-ext="py"><pre><code><span class="line"><span class="token keyword">def</span> <span class="token function">add_prefix</span><span class="token punctuation">(</span>example<span class="token punctuation">)</span><span class="token punctuation">:</span></span>
<span class="line">    example<span class="token punctuation">[</span><span class="token string">&#39;text&#39;</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token string">&#39;My sentence: &#39;</span> <span class="token operator">+</span> example<span class="token punctuation">[</span><span class="token string">&#39;text&#39;</span><span class="token punctuation">]</span></span>
<span class="line">    <span class="token keyword">return</span> example</span>
<span class="line">updated_dataset <span class="token operator">=</span> dataset<span class="token punctuation">.</span><span class="token builtin">map</span><span class="token punctuation">(</span>add_prefix<span class="token punctuation">)</span></span>
<span class="line">updated_dataset<span class="token punctuation">[</span><span class="token string">&#39;train&#39;</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token string">&#39;text&#39;</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token number">5</span><span class="token punctuation">]</span></span>
<span class="line"><span class="token triple-quoted-string string">&#39;&#39;&#39;</span>
<span class="line">示例输出：</span>
<span class="line">[&#39;My sentence: Amrozi accused his brother , whom he called &quot; the witness &quot; , of deliberately distorting his evidence .&#39;,</span>
<span class="line">&quot;My sentence: Yucaipa owned Dominick &#39;s before selling the chain to Safeway in 1998 for $ 2.5 billion .&quot;,</span>
<span class="line">&#39;My sentence: They had published an advertisement on the Internet on June 10 , offering the cargo for sale , he added .&#39;,</span>
<span class="line">&#39;My sentence: Around 0335 GMT , Tab shares were up 19 cents , or 4.4 % , at A $ 4.56 , having earlier set a record high of A $ 4.57 .&#39;,</span>
<span class="line">]</span>
<span class="line">&#39;&#39;&#39;</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>我们首先定义了一个 add_prefix()函数，其作用是为数据集的“text”字段加上一个前缀“My sentence: ”。然后调用数据集 dataset 的 map 方法，可以看到输出中“text”字段的内容前面都增加了指定前缀。</p><p>下面我们来看一看，使用自定义的数据集 fine-tuning BART 模型应该怎么做。具体的代码如下：</p><div class="language-python line-numbers-mode" data-highlighter="prismjs" data-ext="py"><pre><code><span class="line"><span class="token keyword">from</span> transformers<span class="token punctuation">.</span>modeling_bart <span class="token keyword">import</span> shift_tokens_right</span>
<span class="line"></span>
<span class="line">dataset <span class="token operator">=</span> <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span> <span class="token comment"># Datasets的对象，数据集需有&#39;text&#39;和&#39;summary&#39;字段，并包含训练集和验证集</span></span>
<span class="line"></span>
<span class="line"><span class="token keyword">def</span> <span class="token function">convert_to_features</span><span class="token punctuation">(</span>example_batch<span class="token punctuation">)</span><span class="token punctuation">:</span></span>
<span class="line">    input_encodings <span class="token operator">=</span> tokenizer<span class="token punctuation">.</span>batch_encode_plus<span class="token punctuation">(</span>example_batch<span class="token punctuation">[</span><span class="token string">&#39;text&#39;</span><span class="token punctuation">]</span><span class="token punctuation">,</span> pad_to_max_length<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> max_length<span class="token operator">=</span><span class="token number">1024</span><span class="token punctuation">,</span> truncation<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">)</span></span>
<span class="line">    target_encodings <span class="token operator">=</span> tokenizer<span class="token punctuation">.</span>batch_encode_plus<span class="token punctuation">(</span>example_batch<span class="token punctuation">[</span><span class="token string">&#39;summary&#39;</span><span class="token punctuation">]</span><span class="token punctuation">,</span> pad_to_max_length<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> max_length<span class="token operator">=</span><span class="token number">1024</span><span class="token punctuation">,</span> truncation<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">)</span></span>
<span class="line">    </span>
<span class="line">    labels <span class="token operator">=</span> target_encodings<span class="token punctuation">[</span><span class="token string">&#39;input_ids&#39;</span><span class="token punctuation">]</span></span>
<span class="line">    decoder_input_ids <span class="token operator">=</span> shift_tokens_right<span class="token punctuation">(</span>labels<span class="token punctuation">,</span> model<span class="token punctuation">.</span>config<span class="token punctuation">.</span>pad_token_id<span class="token punctuation">)</span></span>
<span class="line">    labels<span class="token punctuation">[</span>labels<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span> <span class="token operator">==</span> model<span class="token punctuation">.</span>config<span class="token punctuation">.</span>pad_token_id<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token operator">-</span><span class="token number">100</span></span>
<span class="line">    </span>
<span class="line">    encodings <span class="token operator">=</span> <span class="token punctuation">{</span></span>
<span class="line">        <span class="token string">&#39;input_ids&#39;</span><span class="token punctuation">:</span> input_encodings<span class="token punctuation">[</span><span class="token string">&#39;input_ids&#39;</span><span class="token punctuation">]</span><span class="token punctuation">,</span></span>
<span class="line">        <span class="token string">&#39;attention_mask&#39;</span><span class="token punctuation">:</span> input_encodings<span class="token punctuation">[</span><span class="token string">&#39;attention_mask&#39;</span><span class="token punctuation">]</span><span class="token punctuation">,</span></span>
<span class="line">        <span class="token string">&#39;decoder_input_ids&#39;</span><span class="token punctuation">:</span> decoder_input_ids<span class="token punctuation">,</span></span>
<span class="line">        <span class="token string">&#39;labels&#39;</span><span class="token punctuation">:</span> labels<span class="token punctuation">,</span></span>
<span class="line">    <span class="token punctuation">}</span></span>
<span class="line"></span>
<span class="line">    <span class="token keyword">return</span> encodings</span>
<span class="line"></span>
<span class="line">dataset <span class="token operator">=</span> dataset<span class="token punctuation">.</span><span class="token builtin">map</span><span class="token punctuation">(</span>convert_to_features<span class="token punctuation">,</span> batched<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span></span>
<span class="line">columns <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">&#39;input_ids&#39;</span><span class="token punctuation">,</span> <span class="token string">&#39;labels&#39;</span><span class="token punctuation">,</span> <span class="token string">&#39;decoder_input_ids&#39;</span><span class="token punctuation">,</span><span class="token string">&#39;attention_mask&#39;</span><span class="token punctuation">,</span><span class="token punctuation">]</span> </span>
<span class="line">dataset<span class="token punctuation">.</span>set_format<span class="token punctuation">(</span><span class="token builtin">type</span><span class="token operator">=</span><span class="token string">&#39;torch&#39;</span><span class="token punctuation">,</span> columns<span class="token operator">=</span>columns<span class="token punctuation">)</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>首先需要加载自定义的数据集，你要注意的是，这个数据集需要包含原文和摘要两个字段，并且包含训练集和验证集。加载数据集的方法可以用我们刚刚讲过的 load_dataset()函数。</p><p>由于加载的数据需要经过一系列预处理操作，比如通过分词器进行分词等等的处理后，才能送入到模型中，因此我们需要定义一个函数 convert_to_features()来处理原文和摘要文本。</p><p>convert_to_features()函数中的主要操作就是调用 tokenizer 来将文本转化为词语 id。需要注意的是，代码第 10 行中有一个 shift_tokens_right()函数，它的作用就是我们在原理中介绍过的 Auto-Regressive，目的是将 Decoder 的输入向后移一个位置。</p><p>然后我们需要调用 dataset.map()函数来对数据集进行预处理操作，参数 batched=True 表示支持在 batch 数据上操作。</p><p>最后再利用 set_format()函数生成选择训练所需的数据字段，并生成 PyTroch 的 Tensor。到这里，数据准备的工作就告一段落了。</p><h3 id="模型训练" tabindex="-1"><a class="header-anchor" href="#模型训练"><span>模型训练</span></a></h3><p>做好了前面的准备工作，最后我们来看模型训练部分。Transformers 工具已经帮我们封装了用于训练文本生成模型的 Seq2SeqTrainer 类，无需我们自己再去定义损失函数与优化方法了。</p><p>具体的训练代码如下。</p><div class="language-python line-numbers-mode" data-highlighter="prismjs" data-ext="py"><pre><code><span class="line"><span class="token keyword">from</span> transformers <span class="token keyword">import</span> Seq2SeqTrainingArguments<span class="token punctuation">,</span> Seq2SeqTrainer</span>
<span class="line"></span>
<span class="line">training_args <span class="token operator">=</span> Seq2SeqTrainingArguments<span class="token punctuation">(</span></span>
<span class="line">    output_dir<span class="token operator">=</span><span class="token string">&#39;./models/bart-summarizer&#39;</span><span class="token punctuation">,</span><span class="token comment"># 模型输出目录</span></span>
<span class="line">    num_train_epochs<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token comment"># 训练轮数</span></span>
<span class="line">    per_device_train_batch_size<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token comment"># 训练过程bach_size</span></span>
<span class="line">    per_device_eval_batch_size<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token comment"># 评估过程bach_size</span></span>
<span class="line">    warmup_steps<span class="token operator">=</span><span class="token number">500</span><span class="token punctuation">,</span> <span class="token comment"># 学习率相关参数</span></span>
<span class="line">    weight_decay<span class="token operator">=</span><span class="token number">0.01</span><span class="token punctuation">,</span> <span class="token comment"># 学习率相关参数</span></span>
<span class="line">    logging_dir<span class="token operator">=</span><span class="token string">&#39;./logs&#39;</span><span class="token punctuation">,</span> <span class="token comment"># 日志目录</span></span>
<span class="line"><span class="token punctuation">)</span></span>
<span class="line"></span>
<span class="line">trainer <span class="token operator">=</span> Seq2SeqTrainer<span class="token punctuation">(</span></span>
<span class="line">    model<span class="token operator">=</span>model<span class="token punctuation">,</span>                       </span>
<span class="line">    args<span class="token operator">=</span>training_args<span class="token punctuation">,</span>                  </span>
<span class="line">    train_dataset<span class="token operator">=</span>dataset<span class="token punctuation">[</span><span class="token string">&#39;train&#39;</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        </span>
<span class="line">    eval_dataset<span class="token operator">=</span>dataset<span class="token punctuation">[</span><span class="token string">&#39;validation&#39;</span><span class="token punctuation">]</span>   </span>
<span class="line"><span class="token punctuation">)</span></span>
<span class="line"></span>
<span class="line">trainer<span class="token punctuation">.</span>train<span class="token punctuation">(</span><span class="token punctuation">)</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>首先我们要定义一个训练参数的对象，关于训练的相关参数都通过 Seq2SeqTrainingArguments 类进行定义。然后再实例化一个 Seq2SeqTrainer 类的对象，将模型和训练数据作为参数传入其中。最后调用 train()方法，即可一键开始训练。</p><h2 id="小结" tabindex="-1"><a class="header-anchor" href="#小结"><span>小结</span></a></h2><p>恭喜你完成了今天的学习任务，同时也完成了 PyTorch 的全部学习内容。</p><p>这节课我们先一起了解了 BART 模型的原理与特点，这个模型是一个非常实用的预训练模型，能够帮助我们实现文本摘要生成。然后我们结合实例，学习了如何用 PyTorch 快速构建一个自动文摘生成项目，包括利用 Transformers 的 pipeline 快速生成文本摘要和 Fine-tuning BART 模型。</p><p>因为 BART 模型具有自回归 Transformer 的结构，所以它不只可以用于摘要生成，还适用于其它文本生成类的项目，例如机器翻译、对话生成等。相信理解了它的基本原理与模型 Fine-tuning 的基本流程，你可以很容易地利用 BART 完成文本生成类的任务，期待你举一反三，亲手做更多的实验。</p><p>通过实战篇的学习，我们一共探讨、实现了 2 个图像项目和 3 个自然语言处理项目。如何基于 PyTorch 搭建自己的深度学习网络，相信你已经了然于胸了。当我们解决实际的问题时，首先要从原理出发，选择适合的模型，PyTorch 只是一个工具，辅助我们实现自己需要的网络。</p><p>除了自动摘要外，其他四个项目的共通思路都是把问题转化为分类问题。图像、文本分类不必细说，图像分割其实是判别一个像素是属于哪一个类别，情感分析则是判别文本是积极类还是消极类。而自动摘要则是生成模型，通常是基于 sequence-to-sequence 的结构来实现。</p><p>这些是我通过一系列的实战训练，最终希望你领会到的模型搭建思路。</p><h2 id="思考题" tabindex="-1"><a class="header-anchor" href="#思考题"><span>思考题</span></a></h2><p>自从 2018 年 BERT 被提出以来，获得了很大的成功，学术界陆续提出了各类相关模型，例如我们今天学习的 BART。请你查一查还有哪些 BERT 系列的模型，并阅读相关论文，自行学习一下它们的原理与特点。</p><p>欢迎你在留言区和我交流互动，也推荐你把这节课转发给更多同事、朋友，跟他一起学习进步。</p>`,84)]))}const c=s(p,[["render",i]]),r=JSON.parse('{"path":"/3.tech/83.PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98/25_%E5%AE%9E%E6%88%98%E7%AF%87-%E6%91%98%E8%A6%81%EF%BC%9A%E5%A6%82%E4%BD%95%E5%BF%AB%E9%80%9F%E5%AE%9E%E7%8E%B0%E8%87%AA%E5%8A%A8%E6%96%87%E6%91%98%E7%94%9F%E6%88%90.html","title":"实战篇-摘要：如何快速实现自动文摘生成","lang":"zh-cn","frontmatter":{},"headers":[{"level":2,"title":"问题背景","slug":"问题背景","link":"#问题背景","children":[{"level":3,"title":"抽取与生成","slug":"抽取与生成","link":"#抽取与生成","children":[]},{"level":3,"title":"评价指标","slug":"评价指标","link":"#评价指标","children":[]}]},{"level":2,"title":"BART 原理与特点分析","slug":"bart-原理与特点分析","link":"#bart-原理与特点分析","children":[]},{"level":2,"title":"快速文摘生成","slug":"快速文摘生成","link":"#快速文摘生成","children":[]},{"level":2,"title":"Fine-tuning BART","slug":"fine-tuning-bart","link":"#fine-tuning-bart","children":[{"level":3,"title":"模型加载","slug":"模型加载","link":"#模型加载","children":[]},{"level":3,"title":"数据准备","slug":"数据准备","link":"#数据准备","children":[]},{"level":3,"title":"模型训练","slug":"模型训练","link":"#模型训练","children":[]}]},{"level":2,"title":"小结","slug":"小结","link":"#小结","children":[]},{"level":2,"title":"思考题","slug":"思考题","link":"#思考题","children":[]}],"git":{"updatedTime":1746672966000,"contributors":[{"name":"guoxin-qiu","username":"guoxin-qiu","email":"guoxin.qiu@outlook.com","commits":3,"url":"https://github.com/guoxin-qiu"}],"changelog":[{"hash":"873191059aa4709eddd6184a409223b5054edb2a","time":1746672966000,"email":"guoxin.qiu@outlook.com","author":"guoxin-qiu","message":"update: pytorch fixed"},{"hash":"f2ddff143d5e7042818e92930c2e210f4b633ca8","time":1746603363000,"email":"guoxin.qiu@outlook.com","author":"guoxin-qiu","message":"update: 250507 bugfix"},{"hash":"b44b80ec6b8c2ebffa55c7b2b54259609c76baed","time":1745668690000,"email":"guoxin.qiu@outlook.com","author":"guoxin-qiu","message":"add pytorch course"}]},"filePathRelative":"3.tech/83.PyTorch深度学习实战/25_实战篇-摘要：如何快速实现自动文摘生成.md"}');export{c as comp,r as data};
