import{_ as s,c as a,a as e,o as p}from"./app-Dmwo-0Oh.js";const t={};function i(l,n){return p(),a("div",null,n[0]||(n[0]=[e(`<h1 id="实战篇-文本分类-如何使用-bert-构建文本分类模型" tabindex="-1"><a class="header-anchor" href="#实战篇-文本分类-如何使用-bert-构建文本分类模型"><span>实战篇-文本分类：如何使用 BERT 构建文本分类模型</span></a></h1><p>在第 22 节课我们一起学习了不少文本处理方面的理论，其实文本分类在机器学习领域的应用也非常广泛。</p><p>比如说你现在是一个 NLP 研发工程师，老板啪地一下甩给你一大堆新闻文本数据，它们可能来源于不同的领域，比如体育、政治、经济、社会等类型。这时我们就需要对文本分类处理，方便用户快速查询自己感兴趣的内容，甚至按用户的需要定向推荐某类内容。</p><p>这样的需求就非常适合用 PyTorch + BERT 处理。为什么会选择 BERT 呢？因为 BERT 是比较典型的深度学习 NLP 算法模型，也是业界使用最广泛的模型之一。接下来，我们就一起来搭建这个文本分类模型，相信我，它的效果表现非常强悍。</p><h2 id="问题背景与分析" tabindex="-1"><a class="header-anchor" href="#问题背景与分析"><span>问题背景与分析</span></a></h2><p>正式动手之前，我们不妨回顾一下历史。文本分类问题有很多经典解决办法。</p><p>开始时就是最简单粗暴的关键词统计方法。之后又有了基于贝叶斯概率的分类方法，通过某些条件发生的概率推断某个类别的概率大小，并作为最终分类的决策依据。尽管这个思想很简单，但是意义重大，时至今日，贝叶斯方法仍旧是非常多应用场景下的好选择。</p><p>之后还有支持向量机（SVM），很长一段时间，其变体和应用都在 NLP 算法应用的问题场景下占据统治地位。</p><p>随着计算设备性能的提升、新的算法理论的产生等进步，一大批的诸如随机森林、LDA 主题模型、神经网络等方法纷纷涌现，可谓百家争鸣。</p><p>既然有这么多方法，为什么这里我们这里推荐选用 BERT 呢？</p><p>因为在很多情况下，尤其是一些复杂场景下的文本，像 BERT 这样具有强大处理能力的工具才能应对。比如说新闻文本就不好分类，因为它存在后面这些问题。</p><p>1.<strong>类别多</strong>。在新闻资讯 App 中，新闻的种类是非常多的，需要产品经理按照统计、实用的原则进行文章分类体系的设计，使其类别能够覆盖所有的文本，一般来说都有 50 种甚至以上。不过为了让你把握重点，咱们先简化问题，假定文本的分类体系已经确定。</p><p>2.<strong>数据不平衡</strong>。不难理解，在新闻中，社会、经济、体育、娱乐等类别的文章数量相对来说是比较多的，占据了很大的比例；而少儿、医疗等类别则相对较少，有的时候一天也没有几篇对应的文章。</p><p>3.**多语言。**一般来说，咱们主要的语言除了中文，应该是大多数人只会英语了，不过为了考虑到新闻来源的广泛性，咱们也假定这批文本是多语言的。</p><p>刚才提到了，因为 Bert 是比较典型的深度学习 NLP 算法模型，也是业界使用最广泛的模型之一。如果拿下这么有代表性的模型，以后你学习和使用基于 Attention 的模型你也能举一反三，比如 GPT 等。</p><p>想要用好 BERT，我们需要先了解它有哪些特点。</p><h2 id="bert-原理与特点分析" tabindex="-1"><a class="header-anchor" href="#bert-原理与特点分析"><span>BERT 原理与特点分析</span></a></h2><p>BERT 的全称是 Bidirectional Encoder Representation from Transformers，即双向 Transformer 的 Encoder。作为一种基于 Attention 方法的模型，它最开始出现的时候可以说是抢尽了风头，在文本分类、自动对话、语义理解等十几项 NLP 任务上拿到了历史最好成绩。</p><p>在[第 22 节课]（如果不熟悉可以回看），我们已经了解了 Attention 的基本原理，有了这个知识做基础，我们很容易就能快速掌握 BERT 的原理。</p><p>这里我再快速给你回顾一下，BERT 的理论框架主要是基于论文《Attention is all you need》中提出的 Transformer，而后者的原理则是刚才提到的 Attention。<strong>其最为明显的特点，就是摒弃了传统的 RNN 和 CNN 逻辑，有效解决了 NLP 中的长期依赖问题。</strong></p><p><img src="https://static001.geekbang.org/resource/image/57/e7/57129ea84051eaf5985535dcb97c1fe7.jpg?wh=1920x1269" alt="图片" title="图片来源：https://arxiv.org/abs/1706.03762"></p><p>在 BERT 中，它的输入部分，也就是图片的左边，其实是由 N 个多头 Attention 组合而成。多头 Attention 是将模型分为多个头，形成多个子空间，可以让模型去关注不同方面的信息，这有助于网络捕捉到更丰富的特征或者信息。（具体原理，一定要查阅<a href="https://arxiv.org/abs/1706.03762" target="_blank" rel="noopener noreferrer">《Attention is all you need》</a>哦）。</p><p>结合上图我们要注意的是，BERT 采用了基于 MLM 的模型训练方式，即 Mask Language Model。因为 BERT 是 Transformer 的一部分，即 encoder 环节，所以没有 decoder 的部分（其实就是 GPT）。</p><p>为了解决这个问题，MLM 方式应运而生。它的思想也非常简单，就是在<strong>训练之前，随机将文本中一部分的词语（token）进行屏蔽（mask），然后在训练的过程中，使用其他没有被屏蔽的 token 对被屏蔽的 token 进行预测</strong>。</p><p><img src="https://static001.geekbang.org/resource/image/ae/84/aeed42d94750436f1dyye31f92c96584.jpg?wh=1920x700" alt="图片"></p><p>用过 Word2Vec 的小伙伴应该比较清楚，在 Word2Vec 中，对于同一个词语，它的向量表示是固定的，这也就是为什么会有那个经典的“<em>国王-男人+女人=皇后</em>”计算式了。</p><p>但是有一个问题，“苹果”这个词，有可能是水果的苹果，也可能是电子产品的品牌，如果还是用同一个向量表示，这样就有可能产生偏差。而在 BERT 中则不一样，根据上下文的不同，对于同一个 token 给出的词向量是动态变化的，更加灵活。</p><p>此外，BERT 还有多语言的优势。在以前的算法中，比如 SVM，如果要做多语言的模型，就要涉及分词、提取关键词等操作，而这些操作要求你对该语言有所了解。像阿拉伯文、日语等语言，咱们大概率是看不懂的，这会对我们最后的模型效果产生极大影响。</p><p>BERT 则不需要担心这个问题，通过基于字符、字符片段、单词等不同粒度的 token 覆盖并作 WordPiece，能够覆盖上百种语言，甚至可以说，只要你能够发明出一种逻辑上自洽的语言，BERT 就能够处理。有关 WordPiece 的介绍，你可以通过<a href="https://paperswithcode.com/method/wordpiece" target="_blank" rel="noopener noreferrer">这里</a>做拓展阅读。</p><p>好，说了这么多，集高效、准确、灵活再加上用途广泛于一体的 BERT，自然而然就成为了咱们的首选，下面咱们开始正式构建一个文本分类模型。</p><h2 id="安装与准备" tabindex="-1"><a class="header-anchor" href="#安装与准备"><span>安装与准备</span></a></h2><p>工欲善其事，必先利其器，在开始构建模型之前，我们要安装相应的工具，然后下载对应的预先训练好的模型，同时还要了解数据的格式。</p><h3 id="环境准备" tabindex="-1"><a class="header-anchor" href="#环境准备"><span>环境准备</span></a></h3><p>因为咱们要做的是一个基于 PyTorch 的 BERT 模型，那么就要安装对应的 python 包，这里我选择的是 hugging face 的 PyTorch 版本的 Transformers 包。你可以通过 pip 命令直接安装。</p><div class="language-text line-numbers-mode" data-highlighter="prismjs" data-ext="text"><pre><code><span class="line">pip install Transformers</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><h3 id="模型准备" tabindex="-1"><a class="header-anchor" href="#模型准备"><span>模型准备</span></a></h3><p>安装之后，我们打开 Transformers 的<a href="https://github.com/huggingface/transformers" target="_blank" rel="noopener noreferrer">git 页面</a>，并找到如下的文件夹。</p><div class="language-plain line-numbers-mode" data-highlighter="prismjs" data-ext="plain"><pre><code><span class="line">src/Transformers/models/BERT</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><p>从这个文件夹里，我们需要找到两个很重要的文件，分别是 convert_BERT_original_tf2_checkpoint_to_PyTorch.py 和 modeling_BERT.py 文件。</p><p>先来看第一个文件，你看看名字，是不是就能猜出来，它大概是用来做什么的了？没错，就是用来将原来通过 TensorfFlow 预训练的模型转换为 PyTorch 的模型。</p><p>然后是 modeling_BERT.py 文件，这个文件实际上是给了你一个使用 BERT 的范例。</p><p>下面，咱们开始准备模型，打开<a href="https://github.com/google-research/bert#pre-trained-models" target="_blank" rel="noopener noreferrer">这个地址</a>，你会发现在这个页面中，有几个预训练好的模型。</p><p><img src="https://static001.geekbang.org/resource/image/f7/a9/f7429816e9c736d99be4b55c67bac6a9.png?wh=1920x956" alt="图片"></p><p>对照这节课的任务，我们选择的是“BERT-Base, Multilingual Cased”的版本。从 GitHub 的介绍可以看出，这个版本的 checkpoint 支持 104 种语言，是不是很厉害？当然，如果你没有多语言的需求，也可以选择其他版本的，它们的区别主要是网络的体积不同。</p><p>转换完模型之后，你会发现你的本地多了三个文件，分别是 config.json、pytorch_model.bin 和 vocab.txt。我来分别给你说一说。</p><p><img src="https://static001.geekbang.org/resource/image/a8/00/a85bfecfb02108cf7e46d5bef74efe00.jpg?wh=1920x228" alt="图片"></p><p>1.config.json：顾名思义，该文件就是 BERT 模型的配置文件，里面记录了所有用于训练的参数设置。</p><p>2.PyTorch_model.bin：模型文件本身。</p><p>3.vocab.txt：词表文件。尽管 BERT 可以处理一百多种语言，但是它仍旧需要词表文件用于识别所支持语言的字符、字符串或者单词。</p><h3 id="格式准备" tabindex="-1"><a class="header-anchor" href="#格式准备"><span>格式准备</span></a></h3><p>现在模型准备好了，我们还要看看跟模型匹配的格式。BERT 的输入不算复杂，但是也需要了解其形式。在训练的时候，我们输入的数据不能是直接把词塞到模型里，而是要转化成后面这三种向量。</p><p>1.<strong>Token embeddings</strong>：词向量。这里需要注意的是，Token embeddings 的第一个开头的 token 一定得是“[CLS]”。[CLS]作为整篇文本的语义表示，用于文本分类等任务。</p><p>2.<strong>Segment embeddings</strong>。这个向量主要是用来将两句话进行区分，比如问答任务，会有问句和答句同时输入，这就需要一个能够区分两句话的操作。不过在咱们此次的分类任务中，只有一个句子。</p><p>3.<strong>Position embeddings</strong>。记录了单词的位置信息。</p><h2 id="模型构建" tabindex="-1"><a class="header-anchor" href="#模型构建"><span>模型构建</span></a></h2><p>准备工作已经一切就绪，我们这就来搭建一个基于 BERT 的文本分类网络模型。这包括了<strong>网络的设计、配置、以及数据准备，这个过程也是咱们的核心过程</strong>。</p><h3 id="网络设计" tabindex="-1"><a class="header-anchor" href="#网络设计"><span>网络设计</span></a></h3><p>从上面提到的 modeling_BERT.py 文件中，我们可以看到，作者实际上已经给我们提供了很多种类的 NLP 任务的示例代码，咱们找到其中的“BERTForSequenceClassification”，这个分类网络我们可以直接使用，它也是最最基础的 BERT 文本分类的流程。</p><p>这个过程包括了利用<strong>BERT 得到文本的 embedding 表示</strong>、<strong>将 embedding 放入全连接层得到分类结果</strong>两部分。我们具体看一下代码。</p><div class="language-python line-numbers-mode" data-highlighter="prismjs" data-ext="py"><pre><code><span class="line"><span class="token keyword">class</span> <span class="token class-name">BERTForSequenceClassification</span><span class="token punctuation">(</span>BERTPreTrainedModel<span class="token punctuation">)</span><span class="token punctuation">:</span></span>
<span class="line">    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> config<span class="token punctuation">)</span><span class="token punctuation">:</span></span>
<span class="line">        <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span>config<span class="token punctuation">)</span></span>
<span class="line">        self<span class="token punctuation">.</span>num_labels <span class="token operator">=</span> config<span class="token punctuation">.</span>num_labels<span class="token operator">//</span>类别标签数量</span>
<span class="line">        self<span class="token punctuation">.</span>bert <span class="token operator">=</span> BertModel<span class="token punctuation">(</span>config<span class="token punctuation">)</span></span>
<span class="line">        self<span class="token punctuation">.</span>dropout <span class="token operator">=</span> nn<span class="token punctuation">.</span>Dropout<span class="token punctuation">(</span>config<span class="token punctuation">.</span>hidden_dropout_prob<span class="token punctuation">)</span><span class="token operator">//</span>还记得Dropout是用来做什么的吗？对，可以一定程度防止过拟合。</span>
<span class="line">        self<span class="token punctuation">.</span>classifier <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>config<span class="token punctuation">.</span>hidden_size<span class="token punctuation">,</span> config<span class="token punctuation">.</span>num_labels<span class="token punctuation">)</span><span class="token operator">//</span>BERT输出的embedding传入一个MLP层做分类。</span>
<span class="line">        self<span class="token punctuation">.</span>init_weights<span class="token punctuation">(</span><span class="token punctuation">)</span></span>
<span class="line"></span>
<span class="line">    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span></span>
<span class="line">        self<span class="token punctuation">,</span></span>
<span class="line">        input_ids<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span></span>
<span class="line">        attention_mask<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span></span>
<span class="line">        token_type_ids<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span></span>
<span class="line">        position_ids<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span></span>
<span class="line">        head_mask<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span></span>
<span class="line">        inputs_embeds<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span></span>
<span class="line">        labels<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span></span>
<span class="line">        output_attentions<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span></span>
<span class="line">        output_hidden_states<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span></span>
<span class="line">        return_dict<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span></span>
<span class="line">    <span class="token punctuation">)</span><span class="token punctuation">:</span></span>
<span class="line">        outputs <span class="token operator">=</span> self<span class="token punctuation">.</span>bert<span class="token punctuation">(</span></span>
<span class="line">            input_ids<span class="token punctuation">,</span></span>
<span class="line">            attention_mask<span class="token operator">=</span>attention_mask<span class="token punctuation">,</span></span>
<span class="line">            token_type_ids<span class="token operator">=</span>token_type_ids<span class="token punctuation">,</span></span>
<span class="line">            position_ids<span class="token operator">=</span>position_ids<span class="token punctuation">,</span></span>
<span class="line">            head_mask<span class="token operator">=</span>head_mask<span class="token punctuation">,</span></span>
<span class="line">            inputs_embeds<span class="token operator">=</span>inputs_embeds<span class="token punctuation">,</span></span>
<span class="line">            output_attentions<span class="token operator">=</span>output_attentions<span class="token punctuation">,</span></span>
<span class="line">            output_hidden_states<span class="token operator">=</span>output_hidden_states<span class="token punctuation">,</span></span>
<span class="line">            return_dict<span class="token operator">=</span>return_dict<span class="token punctuation">,</span></span>
<span class="line">        <span class="token punctuation">)</span></span>
<span class="line"></span>
<span class="line">        pooled_output <span class="token operator">=</span> outputs<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token operator">//</span>这个就是经过BERT得到的中间输出。</span>
<span class="line"></span>
<span class="line">        pooled_output <span class="token operator">=</span> self<span class="token punctuation">.</span>dropout<span class="token punctuation">(</span>pooled_output<span class="token punctuation">)</span><span class="token operator">//</span>对，就是为了减少过拟合和增加网络的健壮性。</span>
<span class="line">        logits <span class="token operator">=</span> self<span class="token punctuation">.</span>classifier<span class="token punctuation">(</span>pooled_output<span class="token punctuation">)</span><span class="token operator">//</span>多层MLP输出最后的分类结果。</span>
<span class="line"></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>对照前面的代码，可以发现，接收到输入信息之后，BERT 返回了一个 outputs，outputs 包括了模型计算之后的全部结果，不仅有每个 token 的信息，也有整个文本的信息，这个输出具体包括以下信息。</p><p>last_hidden_state 是模型最后一层输出的隐藏层状态序列。shape 是(batch_size, sequence_length, hidden_size)。其中 hidden_size=768，这个部分的状态，就相当于利用 sequence_length * 768 维度的矩阵，记录了整个文本的计算之后的每一个 token 的结果信息。</p><p>pooled_output，代表序列的第一个 token 的最后一个隐藏层的状态。shape 是(batch_size, hidden_size)。所谓的第一个 token，就是咱们刚才提到的[CLS]标签。</p><p>除了上面两个信息，还有 hidden_states、attentions、cross attentions。有兴趣的小伙伴可以去查一下，它们有何用途。</p><p>通常的任务中，我们用得比较多的是 last_hidden_state 对应的信息，我们可以用 pooled_output = outputs[1]来进行获取。</p><p>至此，我们已经有了经过 BERT 计算的文本向量表示，然后我们将其输入到一个 linear 层中进行分类，就可以得到最后的分类结果了。<strong>为了提高模型的表现，我们往往会在 linear 层之前，加入一个 dropout 层，这样可以减少网络的过拟合的可能性，同时增强神经元的独立性</strong>。</p><h3 id="模型配置" tabindex="-1"><a class="header-anchor" href="#模型配置"><span>模型配置</span></a></h3><p>设计好网络，我们还要对模型进行配置。还记得刚才提到的 config.json 文件么？这里面就记录了 BERT 模型所需的所有配置信息，我们需要对其中的几个内容进行调整，这样模型就能知道我们到底是要做什么事情了。</p><p>后面这几个字段我专门说一下。</p><ul><li>id2label：这个字段记录了类别标签和类别名称的映射关系。</li><li>label2id：这个字段记录了类别名称和类别标签的映射关系。</li><li>num_labels_cate：类别的数量。</li></ul><h2 id="数据准备" tabindex="-1"><a class="header-anchor" href="#数据准备"><span>数据准备</span></a></h2><p>模型网络设计好了，配置文件也搞定了，下面我们就要开始数据准备这一步了。这里的数据准备是指将文本转换为 BERT 能够识别的形式，即前面提到的三种向量，在代码中，对应的就是 input_ids、token_type_ids、attention_mask。</p><p>为了生成这些数据，我们需要在 git 中找到“src/Transformers/data/processors/utils.py”文件，在这个文件中，我们要用到以下几个内容。</p><p>1.InputExample：它用于记录单个训练数据的文本内容的结构。</p><p>2.DataProcessor：通过这个类中的函数，我们可以将训练数据集的文本，表示为多个 InputExample 组成的数据集合。</p><p>3.get_features：用于把 InputExample 数据转换成 BERT 能够理解的数据结构的关键函数。我们具体来看一下各个数据都怎么生成的。</p><p>input_ids 记录了输入 token 对应在 vocab.txt 的 id 序号，它是通过如下的代码得到的。</p><div class="language-python line-numbers-mode" data-highlighter="prismjs" data-ext="py"><pre><code><span class="line">input_ids <span class="token operator">=</span> tokenizer<span class="token punctuation">.</span>encode<span class="token punctuation">(</span></span>
<span class="line">	                example<span class="token punctuation">.</span>text_a<span class="token punctuation">,</span></span>
<span class="line">	                add_special_tokens<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span></span>
<span class="line">	                max_length<span class="token operator">=</span><span class="token builtin">min</span><span class="token punctuation">(</span>max_length<span class="token punctuation">,</span> tokenizer<span class="token punctuation">.</span>max_len<span class="token punctuation">)</span><span class="token punctuation">,</span></span>
<span class="line">	            <span class="token punctuation">)</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>而 attention_mask 记录了属于第一个句子的 token 信息，通过如下代码得到。</p><div class="language-python line-numbers-mode" data-highlighter="prismjs" data-ext="py"><pre><code><span class="line">attention_mask <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">1</span> <span class="token keyword">if</span> mask_padding_with_zero <span class="token keyword">else</span> <span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">*</span> <span class="token builtin">len</span><span class="token punctuation">(</span>input_ids<span class="token punctuation">)</span></span>
<span class="line">attention_mask <span class="token operator">=</span> attention_mask <span class="token operator">+</span> <span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0</span> <span class="token keyword">if</span> mask_padding_with_zero <span class="token keyword">else</span> <span class="token number">1</span><span class="token punctuation">]</span> <span class="token operator">*</span> padding_length<span class="token punctuation">)</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div></div></div><p>另外，不要忘记记录文本类别的信息（label）。你可以自己想想看，能否按照 utils.py 文件中的声明方式，构建出对应的 label 信息呢？</p><h2 id="模型训练" tabindex="-1"><a class="header-anchor" href="#模型训练"><span>模型训练</span></a></h2><p>到目前为止，我们有了网络结构定义（BERTForSequenceClassification）、数据集合（get_features），现在就可以开始编写实现训练过程的代码了。</p><h3 id="选择优化器" tabindex="-1"><a class="header-anchor" href="#选择优化器"><span>选择优化器</span></a></h3><p>首先我们来选择优化器，代码如下。我们要对网络中的所有权重参数进行设置，这样优化器就可以知道哪些参数是要进行优化的。然后我们将参数 list 放到优化器中，BERT 使用的是 AdamW 优化器。</p><div class="language-plain line-numbers-mode" data-highlighter="prismjs" data-ext="plain"><pre><code><span class="line">param_optimizer = list(model.named_parameters())</span>
<span class="line">no_decay = [&#39;bias&#39;, &#39;LayerNorm.bias&#39;, &#39;LayerNorm.weight&#39;]</span>
<span class="line">optimizer_grouped_parameters = [</span>
<span class="line">            {&#39;params&#39;: [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], &#39;weight_decay&#39;: 0.01},</span>
<span class="line">            {&#39;params&#39;: [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], &#39;weight_decay&#39;: 0.0}</span>
<span class="line">        ]</span>
<span class="line">optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>这部分的代码，主要是为了选择一个合适咱们模型任务的优化器，并将网络中的参数设定好学习率。</p><h3 id="构建训练过程逻辑" tabindex="-1"><a class="header-anchor" href="#构建训练过程逻辑"><span>构建训练过程逻辑</span></a></h3><p>训练的过程逻辑是非常简单的，只需要两个 for 循环，分别代表 epoch 和 batch，然后在最内部增加<strong>一个训练核心语句，<strong>以及</strong>一个梯度更新语句</strong>，这就足够了。可以看到，PyTorch 在工程代码的实现上，封装得非常完善和简练。</p><div class="language-plain line-numbers-mode" data-highlighter="prismjs" data-ext="plain"><pre><code><span class="line">for epoch in trange(0, args.num_train_epochs):</span>
<span class="line">  model.train()//一定别忘了要把模型设置为训练状态。</span>
<span class="line">  for step, batch in enumerate(tqdm(train_dataLoader, desc=&#39;Iteration&#39;)):</span>
<span class="line">    step_loss = training_step(batch)//训练的核心环节</span>
<span class="line">    tr_loss += step_loss[0]</span>
<span class="line">    optimizer.step()</span>
<span class="line">    optimizer.zero_grad()</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h3 id="训练的核心环节" tabindex="-1"><a class="header-anchor" href="#训练的核心环节"><span>训练的核心环节</span></a></h3><p>训练的核心环节，你需要关注两个部分，分别是<strong>通过网络得到预测输出</strong>，也就是 logits，以及<strong>基于 logits 计算得到的 loss</strong>，loss 是整个模型使用梯度更新需要用到的数据。</p><div class="language-plain line-numbers-mode" data-highlighter="prismjs" data-ext="plain"><pre><code><span class="line">def training_step(batch):</span>
<span class="line">  input_ids, token_type_ids, attention_mask, labels = batch</span>
<span class="line">  input_ids = input_ids.to(device)//将数据发送到GPU</span>
<span class="line">  token_type_ids = token_type_ids.to(device)</span>
<span class="line">  attention_mask = attention_mask.to(device)</span>
<span class="line">  labels = labels_voc.to(device)</span>
<span class="line">        </span>
<span class="line">  logits = model(input_ids,</span>
<span class="line">        token_type_ids=token_type_ids, </span>
<span class="line">        attention_mask=attention_mask, </span>
<span class="line">        labels=labels)</span>
<span class="line">  loss_fct = BCEWithLogitsLoss()</span>
<span class="line">  loss = loss_fct(logits.view(-1, num_labels_cate), labels.view(-1, num_labels_cate).float())</span>
<span class="line">  loss.backward()</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>至此，咱们已经快速构建出了一个 BERT 分类器所需的所有关键代码。但是仍旧有一些小小的环节需要你来完善，比如 training_step 代码块中的 device，是怎么得到的呢？回顾一下咱们之前学习的内容，相信你一定可以做得到。</p><h2 id="小结" tabindex="-1"><a class="header-anchor" href="#小结"><span>小结</span></a></h2><p>恭喜你完成了这节课的学习，尽管现在 GitHub 上已经有了很多已经封装得非常完善的 BERT 代码，你也可以很快实现一个最基本的 NLP 算法流程，但是我仍希望你能够抽出时间，好好看一下 Transformer 中的模型代码，这会对你的技术提升有非常大的助益。</p><p>这节课我们学习了如何用 PyTorch 快速构建一个基本的文本分类模型，想要实现这个过程，你需要了解 BERT 的预训练模型的获取以及转化、分类网络的设计方法、训练过程的编写。整个过程不难，但是却可以让你快速上手，了解 PyTorch 在 NLP 方面如何应用。</p><p>除了技术本身，业务方面的考虑我们也要注意。比如新闻文本的多语言、数据不平衡等问题，模型有时不能解决所有的问题，因此你还需要学习一些<strong>数据预处理的技巧</strong>，这包括很多技术和算法方面的内容。</p><p>即使我列出一份长长的学习清单，也可能会挂一漏万，所以数据预处理方面的知识我建议你重点关注以下内容：建议你需要花一些时间去学习 NumPy 和 Pandas 的使用，这样才能更加得心应手地处理数据；你还可以多学习一些常见的数据挖掘算法（比如决策树、KNN、支持向量机等）；另外，深度学习的广泛使用，其实仍旧非常需要传统机器学习算法的背后支撑，也建议你多多了解。</p><h2 id="思考题" tabindex="-1"><a class="header-anchor" href="#思考题"><span>思考题</span></a></h2><p>BERT 处理文本是有最大长度要求的（512），那么遇到长文本，该怎么办呢？</p><p>也欢迎你在留言区记录你的疑问或者收获，也推荐你把这节课分享给你的朋友。</p>`,102)]))}const c=s(t,[["render",i]]),r=JSON.parse('{"path":"/3.tech/83.PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98/24_%E5%AE%9E%E6%88%98%E7%AF%87-%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%EF%BC%9A%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8BERT%E6%9E%84%E5%BB%BA%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%E6%A8%A1%E5%9E%8B.html","title":"实战篇-文本分类：如何使用 BERT 构建文本分类模型","lang":"zh-cn","frontmatter":{},"headers":[{"level":2,"title":"问题背景与分析","slug":"问题背景与分析","link":"#问题背景与分析","children":[]},{"level":2,"title":"BERT 原理与特点分析","slug":"bert-原理与特点分析","link":"#bert-原理与特点分析","children":[]},{"level":2,"title":"安装与准备","slug":"安装与准备","link":"#安装与准备","children":[{"level":3,"title":"环境准备","slug":"环境准备","link":"#环境准备","children":[]},{"level":3,"title":"模型准备","slug":"模型准备","link":"#模型准备","children":[]},{"level":3,"title":"格式准备","slug":"格式准备","link":"#格式准备","children":[]}]},{"level":2,"title":"模型构建","slug":"模型构建","link":"#模型构建","children":[{"level":3,"title":"网络设计","slug":"网络设计","link":"#网络设计","children":[]},{"level":3,"title":"模型配置","slug":"模型配置","link":"#模型配置","children":[]}]},{"level":2,"title":"数据准备","slug":"数据准备","link":"#数据准备","children":[]},{"level":2,"title":"模型训练","slug":"模型训练","link":"#模型训练","children":[{"level":3,"title":"选择优化器","slug":"选择优化器","link":"#选择优化器","children":[]},{"level":3,"title":"构建训练过程逻辑","slug":"构建训练过程逻辑","link":"#构建训练过程逻辑","children":[]},{"level":3,"title":"训练的核心环节","slug":"训练的核心环节","link":"#训练的核心环节","children":[]}]},{"level":2,"title":"小结","slug":"小结","link":"#小结","children":[]},{"level":2,"title":"思考题","slug":"思考题","link":"#思考题","children":[]}],"git":{"updatedTime":1746672966000,"contributors":[{"name":"guoxin-qiu","username":"guoxin-qiu","email":"guoxin.qiu@outlook.com","commits":1,"url":"https://github.com/guoxin-qiu"}],"changelog":[{"hash":"873191059aa4709eddd6184a409223b5054edb2a","time":1746672966000,"email":"guoxin.qiu@outlook.com","author":"guoxin-qiu","message":"update: pytorch fixed"}]},"filePathRelative":"3.tech/83.PyTorch深度学习实战/24_实战篇-文本分类：如何使用BERT构建文本分类模型.md"}');export{c as comp,r as data};
