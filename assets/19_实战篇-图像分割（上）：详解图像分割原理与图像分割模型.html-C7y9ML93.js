import{_ as s,c as a,a as p,o as e}from"./app-Dmwo-0Oh.js";const t={};function c(o,n){return e(),a("div",null,n[0]||(n[0]=[p(`<h1 id="实战篇-图像分割-上-详解图像分割原理与图像分割模型" tabindex="-1"><a class="header-anchor" href="#实战篇-图像分割-上-详解图像分割原理与图像分割模型"><span>实战篇-图像分割（上）：详解图像分割原理与图像分割模型</span></a></h1><p>在前两节课我们完成了有关图像分类的学习与实践。今天，让我们进入到计算机视觉另外一个非常重要的应用场景——图像分割。</p><p>你一定用过或听过腾讯会议或者 Zoom 之类的产品吧?在进行会议的时候，我们可以选择对背景进行替换，如下图所示。</p><p><img src="https://static001.geekbang.org/resource/image/30/82/3066670d30116f462e54fd50376f5882.png?wh=1282x878" alt="图片" title="图片来源：https://tech.qq.com/a/20200426/002647.htm"></p><p>在华为手机中也曾经有过人像留色的功能。</p><p><img src="https://static001.geekbang.org/resource/image/b3/ec/b33ecbc167f2bbf66902cb35cc9e3eec.png?wh=1272x862" alt="图片" title="图片来源：https://www.sohu.com/a/294693393_264578"></p><p>这些应用背后的实现都离不开今天要讲的图像分割。</p><p>我们同样用两节课的篇幅进行学习，这节课主攻分割原理，下节课再把这些技能点活用到实战上，从头开始搭建一个图像分割模型。</p><h2 id="图像分割" tabindex="-1"><a class="header-anchor" href="#图像分割"><span>图像分割</span></a></h2><p>我们不妨用对比的视角，先从概念理解一下图像分割是什么。图像分类是将一张图片自动分成某一类别，而图像分割是需要将图片中的每一个像素进行分类。</p><p>图像分割可以分为语义分割与实例分割，两者的区别是语义分割中只需要把每个像素点进行分类就可以了，不需要区分是否来自同一个实例，而实例分割不仅仅需要对像素点进行分类，还需要判断来自哪个实例。</p><p>如下图所示，左侧为语义分割，右侧为实例分割。我们这两节课都会以语义分割来展开讲解。</p><p><img src="https://static001.geekbang.org/resource/image/75/81/75d04920aa9208d0108fd4e35332e281.png?wh=1622x540" alt="图片"></p><h2 id="语义分割原理" tabindex="-1"><a class="header-anchor" href="#语义分割原理"><span>语义分割原理</span></a></h2><p>语义分割原理其实与图像分类大致类似，主要有两点区别。首先是分类端（这是我自己起的名字，就是经过卷积提取特征后分类的那一块）不同，其次是网络结构有所不同。先看第一点，也就是分类端的不同。</p><h3 id="分类端" tabindex="-1"><a class="header-anchor" href="#分类端"><span>分类端</span></a></h3><p>我们先回想一下图像分类的原理。你可以结合下面的示意图做理解。</p><p><img src="https://static001.geekbang.org/resource/image/39/8a/39a58yy024069706401e741b63bd808a.jpg?wh=1699x914" alt="图片"></p><p>输入图片经过卷积层提取特征后，最终会生成若干特征图，然后在这些特征图之后会接一个全连接层（上图中红色的圆圈），全连接层中的节点数就对应着要将图片分为几类。我们将全连接层的输出送入到 softmax 中，就可以获得每个类别的概率，然后通过概率就可以判断输入图片属于哪一个类别了。</p><p>在图像分割中，同样是利用卷积层来提取特征，最终生成若干特征图。只不过最后生成的特征图</p><p>的数目对应着要分割成的类别数。举一个例子，假设我们想要将输入的小猫分割出来，也就是说，这个图像分割模型有两个类别，分别是小猫与背景，如下图所示。</p><p><img src="https://static001.geekbang.org/resource/image/32/66/325b9e7e91200c1e9fba3e6a985dbc66.jpg?wh=1392x875" alt="图片"></p><p>最终的两个特征图中，通道 1 代表的小猫的信息，通道 2 对应着背景的信息。</p><p>这里我给你再举一个例子，来说明一下如何判断每个像素的类别。假设，通道 1 中（0,0）这个位置的输出是 2，通道 2 中（0,0）这个位置的输出是 30。</p><p>经过 softmax 转为概率后，通道 1（0, 0）这个位置的概率为 0，而对应通道 2 中(0,0)这个位置的概率为 1，我们通过概率可以判断出，在（0，0）这个位置是背景，而不是小猫。</p><h3 id="网络结构" tabindex="-1"><a class="header-anchor" href="#网络结构"><span>网络结构</span></a></h3><p>在分割网络中最终输出的特征图的大小要么是与输入的原图相同，要么就是接近输入。</p><p>这么做的原因是，我们要对原图中的每个像素进行判断。当输出特征图与原图尺寸相同时，可以直接进行分割判断。当输出特征图与原图尺寸不相同时，需要将输出的特征图 resize 到原图大小。</p><p>如果是从一个比较小的特征图 resize 到一个比较大的尺寸的时候，必定会丢失掉一部分信息的。所以，输出特征图的大小不能太小。</p><p>这也是图像分割网络与图像分类网络的第二个不同点，在图像分类中，经过多层的特征提取，最后生成的特征图都是很小的。而<strong>在图像分割中，最后生成的特征图通常来说是接近原图的</strong>。</p><p>前文也说过，图像分割网络也是通过卷积进行提取特征的，按照之前的理论特征提取后，特征图尺寸是减小的。如果说把特征提取看做 Encoder 的话，那在图像分割中还有一步是 Decoder。</p><p>Decoder 的作用就是对特征图尺寸进行还原的过程，将尺寸还原到一个比较大的尺寸。这个还原的操作对应的就是上采样。而在上采样中我们通常使用的是转置卷积。</p><h4 id="转置卷积" tabindex="-1"><a class="header-anchor" href="#转置卷积"><span>转置卷积</span></a></h4><p>接下来我就带你研究一下转置卷积的计算原理，这也是这节课的重点内容。</p><p>我们看下面图这个卷积计算，padding 为 0，stride 为 1。</p><p><img src="https://static001.geekbang.org/resource/image/ac/c7/ac5dfca8d13e88b3e78fb3f8b34016c7.jpg?wh=1520x865" alt="图片"></p><p>从之前的学习我们可以知道，卷积操作是一个多对一的运算，输出中的每一个 y 都与输入中的 4 个 x 有关。其实，转置卷积从逻辑上是卷积的一个逆过程，而<strong>不是卷积的逆运算。</strong></p><p>也就是说，转置卷积并不是使用上图中的输出 Y 与卷积核 Kernel 来获得上图中的输入 X，转置卷积只能还原出一个与输入特征图<strong>尺寸</strong>相同的特征图。</p><p>我们将转置卷积中的卷积核用 k&#39;表示，那么一个 y 会与四个 k&#39;进行还原，如下所示：</p><p><img src="https://static001.geekbang.org/resource/image/9d/2d/9d24b6a621f2b74648248dbce723a52d.jpg?wh=1593x745" alt="图片"></p><p>还原尺寸的过程如下所示，下图中每个还原后的结果都对应着原始 3x3 的输入。</p><p><img src="https://static001.geekbang.org/resource/image/1c/f1/1c18d2dbc330062410fddbcc911f78f1.jpg?wh=1920x1080" alt=""></p><p>通过观察你可以发现，有些部分是重合的，对于重合部分把它们加起来就可以了，最终还原后的特征图如下：</p><p><img src="https://static001.geekbang.org/resource/image/41/01/41833998c1510d0a6c1fe239a0557101.jpg?wh=1920x1110" alt="图片"></p><p>将上图的结果稍作整理，整理为下面的结果，也没有做什么特殊处理，只是补了一些零：</p><p><img src="https://static001.geekbang.org/resource/image/b7/0f/b7078cdd1ce155d1bb853ab2a88c000f.jpg?wh=1920x1163" alt=""></p><p>上面的结果，我们又可以通过下面的卷积获得： <img src="https://static001.geekbang.org/resource/image/64/bb/640099fe3138893e0a9f6941c0d49bbb.jpg?wh=1920x980" alt=""></p><p>你有没有发现一件很神奇的事情，转置卷积计算又变回了卷积计算。</p><p>所以，我们一起梳理一下，转置卷积的计算过程如下：</p><p>1.对输入特征图进行补零操作。 2.将转置卷积的卷积核上下、左右变换作为新的卷积核。 3.利用新的卷积核在 1 的基础上进行<strong>步长为 1</strong>，<strong>padding 为 0</strong>的卷积操作。</p><p>我们先来看一下，PyTorch 中转置卷积以及它的主要参数，再根据参数解释一下第一步 1 是如何补零的。</p><div class="language-python line-numbers-mode" data-highlighter="prismjs" data-ext="py"><pre><code><span class="line"><span class="token keyword">class</span> <span class="token class-name">torch</span><span class="token punctuation">.</span>nn<span class="token punctuation">.</span>ConvTranspose2d<span class="token punctuation">(</span>in_channels<span class="token punctuation">,</span></span>
<span class="line">                               out_channels<span class="token punctuation">,</span></span>
<span class="line">                               kernel_size<span class="token punctuation">,</span></span>
<span class="line">                               stride<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span></span>
<span class="line">                               padding<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span></span>
<span class="line">                               groups<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span></span>
<span class="line">                               bias<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span></span>
<span class="line">                               dilation<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>其中，in_channels、out_channels、kernel_size、groups、bias 以及 dilation 与我们之前讲卷积时的参数含义是一样的（你可以回顾卷积的第<a href="https://time.geekbang.org/column/article/432042" target="_blank" rel="noopener noreferrer">9</a>、<a href="https://time.geekbang.org/column/article/433801" target="_blank" rel="noopener noreferrer">10</a>两节课），这里我们就不赘述了。</p><p>首先，我们看一下 stride。因为转置卷积是卷积的一个逆向过程，所以这里的 stride 指的是在原图上的 stride。</p><p>在我们刚才的例子里，stride 是等于 1 的，如果等于 2 时，按照同样的套路，可以转换为如下的卷积变换。 同时，我们也可以得到结论，上文中第一步，补零的操作是，在输入的特征图的行与列之间补 stride-1 个行与列的零。</p><p><img src="https://static001.geekbang.org/resource/image/39/ca/3913cdcc21afe55ccb8511951c4512ca.jpg?wh=1920x1011" alt="图片"></p><p>再来看 padding 操作，padding 是指要在输入特征图的周围补 dilation * (kernel_size - 1) - padding 圈零。这里用到了 dliation 参数，但是通常在转置卷积中 dilation、groups 参数使用的比较少。</p><p>以上就是转置卷积的补零操作了，图片和文字双管齐下，我相信你一定能够理解它。</p><p>通过上述的讲解，我们可以推导出输出特征图尺寸与输入特征图尺寸的关系：</p><p>$$h_{out} = (h_{in} - 1) * stride[0] - padding[0] + kernel\\_size[0]$$</p><p>$$w_{out} = (w_{in} - 1) * stride[1] - padding[1] + kernel\\_size[1]$$</p><p>下面，我们借助代码来验证一下，我们讲的转置卷积是否是向我们所说的那样计算。</p><p>现在有特征图 input_feat:</p><div class="language-python line-numbers-mode" data-highlighter="prismjs" data-ext="py"><pre><code><span class="line"><span class="token keyword">import</span> torch</span>
<span class="line"><span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn</span>
<span class="line"><span class="token keyword">import</span> numpy <span class="token keyword">as</span> np</span>
<span class="line">input_feat <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>float32<span class="token punctuation">)</span></span>
<span class="line">input_feat</span>
<span class="line">输出：</span>
<span class="line">tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1.</span><span class="token punctuation">,</span> <span class="token number">2.</span><span class="token punctuation">]</span><span class="token punctuation">,</span></span>
<span class="line">          <span class="token punctuation">[</span><span class="token number">3.</span><span class="token punctuation">,</span> <span class="token number">4.</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>卷积核 k：</p><div class="language-python line-numbers-mode" data-highlighter="prismjs" data-ext="py"><pre><code><span class="line">kernels <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>float32<span class="token punctuation">)</span></span>
<span class="line">kernels</span>
<span class="line">输出：</span>
<span class="line">tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1.</span><span class="token punctuation">,</span> <span class="token number">0.</span><span class="token punctuation">]</span><span class="token punctuation">,</span></span>
<span class="line">          <span class="token punctuation">[</span><span class="token number">1.</span><span class="token punctuation">,</span> <span class="token number">1.</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span></span>
<span class="line"></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>stride 为 1，padding 为 0 的转置卷积：</p><div class="language-python line-numbers-mode" data-highlighter="prismjs" data-ext="py"><pre><code><span class="line">convTrans <span class="token operator">=</span> nn<span class="token punctuation">.</span>ConvTranspose2d<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> bias <span class="token operator">=</span> <span class="token boolean">False</span><span class="token punctuation">)</span></span>
<span class="line">convTrans<span class="token punctuation">.</span>weight<span class="token operator">=</span>nn<span class="token punctuation">.</span>Parameter<span class="token punctuation">(</span>kernels<span class="token punctuation">)</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div></div></div><p>按照我们刚才讲的，第一步是补零操作，输入的特征图补零后为：</p><p>$$ input\\_feat = \\begin{bmatrix} 0 &amp; 0 &amp; 0 &amp; 0 \\\\\\ 0 &amp; 1 &amp; 2 &amp; 0 \\\\\\ 0 &amp; 3 &amp; 4 &amp; 0 \\\\\\ 0 &amp; 0 &amp; 0 &amp; 0\\\\\\ \\end{bmatrix} $$</p><p>然后再与变换后的卷积核：</p><p>$$ \\begin{bmatrix} 1 &amp; 1 \\\\\\ 0 &amp; 1 \\end{bmatrix} $$</p><p>做卷积运算后，获得输出：</p><p>$$ output = \\begin{bmatrix} 1 &amp; 2 &amp; 0 \\\\\\ 4 &amp; 7 &amp; 2 \\\\\\ 3 &amp; 7 &amp; 4 \\end{bmatrix} $$</p><p>我们再看看代码的输出，如下所示：</p><div class="language-python line-numbers-mode" data-highlighter="prismjs" data-ext="py"><pre><code><span class="line">convTrans<span class="token punctuation">(</span>input_feat<span class="token punctuation">)</span></span>
<span class="line">输出：</span>
<span class="line">tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1.</span><span class="token punctuation">,</span> <span class="token number">2.</span><span class="token punctuation">,</span> <span class="token number">0.</span><span class="token punctuation">]</span><span class="token punctuation">,</span></span>
<span class="line">          <span class="token punctuation">[</span><span class="token number">4.</span><span class="token punctuation">,</span> <span class="token number">7.</span><span class="token punctuation">,</span> <span class="token number">2.</span><span class="token punctuation">]</span><span class="token punctuation">,</span></span>
<span class="line">          <span class="token punctuation">[</span><span class="token number">3.</span><span class="token punctuation">,</span> <span class="token number">7.</span><span class="token punctuation">,</span> <span class="token number">4.</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span> grad_fn<span class="token operator">=</span><span class="token operator">&lt;</span>SlowConvTranspose2DBackward<span class="token operator">&gt;</span><span class="token punctuation">)</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>你看看是不是一样呢？</p><h3 id="损失函数" tabindex="-1"><a class="header-anchor" href="#损失函数"><span>损失函数</span></a></h3><p>说完网络结构，我们再开启图像分割里的另一个话题：损失函数。</p><p>在图像分割中依然可以使用在图像分类中经常使用的交叉熵损失。在图像分类中，一张图片有一个预测结果，预测结果与真实值就可以计算出一个 Loss。而在图像分割中，真实的标签是一张二维特征图，这张特征图记录着每个像素的真实分类结果。在分割中，含有像素类别的特征图，我们一般称为 Mask。</p><p>我们结合一张小猫图片的例子解释一下。对于下图中的小猫进行标记，标记后会生成它的 GT，这个 GT 就是一个 Mask。</p><p>GT 是 Ground Truth 的缩写，在图像分割中我们经常使用这个词。在图像分类中与之对应的就是数据的真实标签，在图像分割中则 GT 是每个像素的真实分类，如下面的例子所示。</p><p><img src="https://static001.geekbang.org/resource/image/c2/db/c258c4f2ffd1f819c662aa1e9f6a8cdb.jpg?wh=1024x640" alt="图片"></p><p>GT 如下所示：</p><p><img src="https://static001.geekbang.org/resource/image/1a/a0/1a35623ceccb0750cd8058568d847fa0.png?wh=1024x640" alt="图片"></p><p>那在我们模型预测的 Mask 中，每个位置都会有一个预测结果，这个预测结果与 GT 中的 Mask 做比较，然后会生成一个 Loss。</p><p>当然，在图像分割中不光有交叉熵损失可以用，还可以用更加有针对性的 Dice Loss，下节课我再继续展开。</p><h4 id="公开数据集" tabindex="-1"><a class="header-anchor" href="#公开数据集"><span>公开数据集</span></a></h4><p>刚才我们也看到了，图像分割的数据标注还是比较耗时的，具体如何标注一张语义分割所需要的图片，下节课我们再一起通过实践探索。</p><p>除此之外业界也有很多比较有权威性且质量很高的公开数据集。最著名的就是 COCO 了，链接如下：<a href="https://cocodataset.org/#detection-2016" target="_blank" rel="noopener noreferrer">https://cocodataset.org/#detection-2016</a>。一共有 80 个类别，超过 2 万张图片。感兴趣的话，课后你可以尝试着使用它训练来看看。</p><p><img src="https://static001.geekbang.org/resource/image/60/f7/6027c11940b8e0205b182505a371c0f7.png?wh=1598x348" alt="图片"></p><h2 id="小结" tabindex="-1"><a class="header-anchor" href="#小结"><span>小结</span></a></h2><p>恭喜你完成了今天的学习。</p><p>今天我们首先明确了语义分割要解决的问题是什么，它可以对图像中的每个像素都进行分类。</p><p>然后我们对比图像分类原理，说明了语义分割的原理。它与图像分类主要有两个不同点：</p><p>1.在分类端有所不同，在图像分类中，经过卷积的特征提取后，最后会以若干个神经元的形式作为输出，每个神经元代表着对一个类别的判断情况。而语义分割，则是会输出若干的特征图，每个特征图代表着对应类别判断。</p><p>2.在图像分类的网络中，特征图是不断减小的。但是在语义分割的网络中，特征图还会有 decoder 这一步，它是将特征图进行放大的过程。实现 decoder 的方式称为上采样，在上采样中我们最常使用的就是转置卷积。</p><p>对于转置卷积，我们除了要知道它是怎么计算的之外，最重要的是要记住<strong>它不是卷积的逆运算，只是能将特征图的大小进行放大的一种卷积运算</strong>。</p><h2 id="每课一练" tabindex="-1"><a class="header-anchor" href="#每课一练"><span>每课一练</span></a></h2><p>对于本文的小猫分割问题，最终只输出 1 个特征图是否可以？</p><p>欢迎你在留言区跟我交流互动，也推荐你把这节课分享给更多同事、朋友。</p>`,101)]))}const l=s(t,[["render",c]]),u=JSON.parse('{"path":"/3.tech/83.PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98/19_%E5%AE%9E%E6%88%98%E7%AF%87-%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2%EF%BC%88%E4%B8%8A%EF%BC%89%EF%BC%9A%E8%AF%A6%E8%A7%A3%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2%E5%8E%9F%E7%90%86%E4%B8%8E%E5%9B%BE%E5%83%8F%E5%88%86%E5%89%B2%E6%A8%A1%E5%9E%8B.html","title":"实战篇-图像分割（上）：详解图像分割原理与图像分割模型","lang":"zh-cn","frontmatter":{},"headers":[{"level":2,"title":"图像分割","slug":"图像分割","link":"#图像分割","children":[]},{"level":2,"title":"语义分割原理","slug":"语义分割原理","link":"#语义分割原理","children":[{"level":3,"title":"分类端","slug":"分类端","link":"#分类端","children":[]},{"level":3,"title":"网络结构","slug":"网络结构","link":"#网络结构","children":[]},{"level":3,"title":"损失函数","slug":"损失函数","link":"#损失函数","children":[]}]},{"level":2,"title":"小结","slug":"小结","link":"#小结","children":[]},{"level":2,"title":"每课一练","slug":"每课一练","link":"#每课一练","children":[]}],"git":{"updatedTime":1746672966000,"contributors":[{"name":"guoxin-qiu","username":"guoxin-qiu","email":"guoxin.qiu@outlook.com","commits":2,"url":"https://github.com/guoxin-qiu"}],"changelog":[{"hash":"873191059aa4709eddd6184a409223b5054edb2a","time":1746672966000,"email":"guoxin.qiu@outlook.com","author":"guoxin-qiu","message":"update: pytorch fixed"},{"hash":"b44b80ec6b8c2ebffa55c7b2b54259609c76baed","time":1745668690000,"email":"guoxin.qiu@outlook.com","author":"guoxin-qiu","message":"add pytorch course"}]},"filePathRelative":"3.tech/83.PyTorch深度学习实战/19_实战篇-图像分割（上）：详解图像分割原理与图像分割模型.md"}');export{l as comp,u as data};
