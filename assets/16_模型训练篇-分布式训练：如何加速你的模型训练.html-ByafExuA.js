import{_ as s,c as a,a as p,o as e}from"./app-Dmwo-0Oh.js";const t={};function l(o,n){return e(),a("div",null,n[0]||(n[0]=[p(`<h1 id="模型训练篇-分布式训练-如何加速你的模型训练" tabindex="-1"><a class="header-anchor" href="#模型训练篇-分布式训练-如何加速你的模型训练"><span>模型训练篇-分布式训练：如何加速你的模型训练</span></a></h1><p>在之前的课程里，我们一起学习了深度学习必备的内容，包括构建网络、损失函数、优化方法等，这些环节掌握好了，我们就可以训练很多场景下的模型了。</p><p>但是有的时候，我们的模型比较大，或者训练数据比较多，训练起来就会比较慢，该怎么办呢？这时候牛气闪闪的分布式训练登场了，有了它，我们就可以极大地加速我们的训练过程。</p><p>这节课我就带你入门分布式训练，让你吃透分布式训练的工作原理，最后我还会结合一个实战项目，带你小试牛刀，让你在动手过程中加深对这部分内容的理解。</p><h2 id="分布式训练原理" tabindex="-1"><a class="header-anchor" href="#分布式训练原理"><span>分布式训练原理</span></a></h2><p>在具体介绍分布式训练之前，我们需要先简要了解一下为什么深度学习要使用 GPU。</p><p>在我们平时使用计算机的时候，程序都是将进程或者线程的数据资源放在内存中，然后在 CPU 进行计算。通常的程序中涉及到了大量的 if else 等分支逻辑操作，这也是 CPU 所擅长的计算方式。</p><p>而在深度学习中，模型的训练与计算过程则没有太多的分支，基本上都是矩阵或者向量的计算，而这种暴力又单纯的计算形式非常适合用 GPU 处理，GPU 的整个处理过程就是一个流式处理的过程。</p><p>但是再好的车子，一个缸的发动机也肯定比不过 12 个缸的，同理单单靠一个 GPU，速度肯定还是不够快，于是就有了多个 GPU 协同工作的办法，即分布式训练。分布式训练，顾名思义就是训练的过程是分布式的，重点就在于后面这两个问题：</p><p>1.谁分布了？答案有两个：数据与模型。 2.怎么分布？答案也有两个：单机多卡与多机多卡。</p><p>也就是说，为了实现深度学习的分布式训练，我们需要采用单机多卡或者多机多卡的方式，让分布在不同 GPU 上的数据和模型协同训练。那么接下来，我们先从简单的单机单卡入手，了解一下 GPU 的训练过程。</p><h3 id="单机单卡" tabindex="-1"><a class="header-anchor" href="#单机单卡"><span>单机单卡</span></a></h3><p>想象一下，如果让你把数据或者模型推送到 GPU 上，需要做哪几步操作呢？让我们先从单 GPU 的情况出发。</p><p>第一步，我们需要知道手头有多少 GPU。PyTorch 中使用 torch.cuda.is_available()函数来判断当前的机器是否有可用的 GPU，而函数 torch.cuda.device_count()则可以得到目前可用的 GPU 的数量。</p><p>第二步，获得 GPU 的一个实例。例如下面的语句：</p><div class="language-python line-numbers-mode" data-highlighter="prismjs" data-ext="py"><pre><code><span class="line">device <span class="token operator">=</span> torch<span class="token punctuation">.</span>device<span class="token punctuation">(</span><span class="token string">&quot;cuda:0&quot;</span> <span class="token keyword">if</span> torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>is_available<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">else</span> <span class="token string">&quot;cpu&quot;</span><span class="token punctuation">)</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><p>这里 torch.device 代表将 torch.Tensor 分配到的设备，是一个设备对象实例，也就是 GPU。其中 cuda: 0 表示我们使用的是第一块 GPU。当然你也可以不用声明“:0”，默认就从第一块开始。如果没有 GPU（torch.cuda.is_available()），那就只能使用 CPU 了。</p><p>第三步，将数据或者模型推到 GPU 上去，这个过程我们称为<strong>迁移</strong>。</p><p>在 PyTorch 中，这个过程的封装程度非常高，换句话说，我们只需要保证即将被推到 GPU 的内容是张量（Tensor）或者模型（Module），就可以用 to()函数快速进行实现。例如：</p><div class="language-python line-numbers-mode" data-highlighter="prismjs" data-ext="py"><pre><code><span class="line">data <span class="token operator">=</span> torch<span class="token punctuation">.</span>ones<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">)</span></span>
<span class="line"><span class="token keyword">print</span><span class="token punctuation">(</span>data<span class="token punctuation">.</span>device<span class="token punctuation">)</span></span>
<span class="line"><span class="token comment"># Get: cpu</span></span>
<span class="line"></span>
<span class="line"><span class="token comment"># 获得device</span></span>
<span class="line">device <span class="token operator">=</span> torch<span class="token punctuation">.</span>device<span class="token punctuation">(</span><span class="token string">&quot;cuda: 0&quot;</span><span class="token punctuation">)</span></span>
<span class="line"></span>
<span class="line"><span class="token comment"># 将data推到gpu上</span></span>
<span class="line">data_gpu <span class="token operator">=</span> data<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span></span>
<span class="line"><span class="token keyword">print</span><span class="token punctuation">(</span>data_gpu<span class="token punctuation">.</span>device<span class="token punctuation">)</span></span>
<span class="line"><span class="token comment"># Get: cuda:0</span></span>
<span class="line"></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>在上面这段代码中，我们首先创建了一个常规的张量 data，通过 device 属性，可以看到 data 现在是在 CPU 上的。随后，我们通过 to()函数将 data 迁移到 GPU 上，同样也能通过 device 属性看到 data 确实已经存在于 GPU 上了。</p><p>那么对于模型，是否也是一样的操作呢？答案是肯定的，我们接下来看一个例子：</p><div class="language-python line-numbers-mode" data-highlighter="prismjs" data-ext="py"><pre><code><span class="line">net <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">)</span></span>
<span class="line">net<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div></div></div><p>这里仍旧使用 to()函数即可。</p><p>单机单卡的模式，相当于有一批要处理加工的产品，只分给了一个工人和一台机器来完成，这种情况下数量少了还可以，但是一旦产品太多了，就得加人、加机器才能快速交工了。</p><p>深度学习也是一样，在很多场景中，比如推荐算法模型、语言模型等，往往都有着百万、千万甚至上亿的训练数据，这样如果只用一张卡的话肯定是搞不定了。于是就有了单机多卡和多机多卡的解决方案。</p><h3 id="单机多卡" tabindex="-1"><a class="header-anchor" href="#单机多卡"><span>单机多卡</span></a></h3><p>那么，在 PyTorch 中，单机多卡的训练是如何进行的呢？其实 PyTorch 提供了好几种解决方案，咱们先看一个最简单也是最常用的办法：nn.DataParallel()。其具体定义如下：</p><div class="language-python line-numbers-mode" data-highlighter="prismjs" data-ext="py"><pre><code><span class="line">torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>DataParallel<span class="token punctuation">(</span>module<span class="token punctuation">,</span> device_ids<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> output_device<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><p>在这里，module 就是你定义的模型，device_ids 即为训练模型时用到的 GPU 设备号，output_device 表示输出结果的 device，默认为 0 也就是第一块卡。</p><p>我们可以使用 nvidia-smi 命令查看 GPU 使用情况。如果你足够细心就会发现，使用多个卡做训练的时候，output_device 的卡所占的显存明显大一些。</p><p>继续观察你还会发现，使用 DataParallel 时，数据的使用是并行的，每张卡获得的数据都一样多，但是输出的 loss 则是所有的卡的 loss 都会在第 output_device 块 GPU 进行计算，这导致了 output_device 卡的负载进一步增加。</p><p><img src="https://static001.geekbang.org/resource/image/7f/08/7f8e9a83fa6a91yyf931565c55f0a708.png?wh=1130x522" alt="图片"></p><p>就这么简单？对，就这么简单，只需要一个 DataParallel 函数就可以将模型分发到多个 GPU 上。但是我们还是需要了解这内部的运行逻辑，因为只有了解了这个逻辑，在以后的开发中遇到了诸如<strong>时间计算、资源预估、优化调试问题</strong>的时候，你才可以更好地运用 GPU，让多 GPU 的优势真正发挥出来。</p><p>在模型的前向计算过程中，数据会被划分为多个块，被推送到不同的 GPU 进行计算。但是不同的是，模型在每个 GPU 中都会复制一份。我们看一下后面的代码：</p><div class="language-python line-numbers-mode" data-highlighter="prismjs" data-ext="py"><pre><code><span class="line"><span class="token keyword">class</span> <span class="token class-name">ASimpleNet</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span></span>
<span class="line">    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> layers<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">:</span></span>
<span class="line">        <span class="token builtin">super</span><span class="token punctuation">(</span>ASimpleNet<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span></span>
<span class="line">        self<span class="token punctuation">.</span>linears <span class="token operator">=</span> nn<span class="token punctuation">.</span>ModuleList<span class="token punctuation">(</span><span class="token punctuation">[</span>nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span> <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>layers<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span></span>
<span class="line">    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span></span>
<span class="line">        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&quot;forward batchsize is: {}&quot;</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>x<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span></span>
<span class="line">        x <span class="token operator">=</span> self<span class="token punctuation">.</span>linears<span class="token punctuation">(</span>x<span class="token punctuation">)</span></span>
<span class="line">        x <span class="token operator">=</span> torch<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>x<span class="token punctuation">)</span></span>
<span class="line">        <span class="token keyword">return</span> x</span>
<span class="line"></span>
<span class="line">batch_size <span class="token operator">=</span> <span class="token number">16</span></span>
<span class="line">inputs <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span>batch_size<span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span></span>
<span class="line">labels <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span>batch_size<span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span></span>
<span class="line">inputs<span class="token punctuation">,</span> labels <span class="token operator">=</span> inputs<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span><span class="token punctuation">,</span> labels<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span></span>
<span class="line">net <span class="token operator">=</span> ASimpleNet<span class="token punctuation">(</span><span class="token punctuation">)</span></span>
<span class="line">net <span class="token operator">=</span> nn<span class="token punctuation">.</span>DataParallel<span class="token punctuation">(</span>net<span class="token punctuation">)</span></span>
<span class="line">net<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span></span>
<span class="line"><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&quot;CUDA_VISIBLE_DEVICES :{}&quot;</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>os<span class="token punctuation">.</span>environ<span class="token punctuation">[</span><span class="token string">&quot;CUDA_VISIBLE_DEVICES&quot;</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span></span>
<span class="line"></span>
<span class="line"><span class="token keyword">for</span> epoch <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">:</span></span>
<span class="line">    outputs <span class="token operator">=</span> net<span class="token punctuation">(</span>inputs<span class="token punctuation">)</span></span>
<span class="line"></span>
<span class="line"><span class="token comment"># Get:</span></span>
<span class="line"><span class="token comment"># CUDA_VISIBLE_DEVICES : 3, 2, 1, 0</span></span>
<span class="line"><span class="token comment"># forward batchsize is: 4</span></span>
<span class="line"><span class="token comment"># forward batchsize is: 4</span></span>
<span class="line"><span class="token comment"># forward batchsize is: 4</span></span>
<span class="line"><span class="token comment"># forward batchsize is: 4</span></span>
<span class="line"></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>在上面的程序中，我们通过 CUDA_VISIBLE_DEVICES 得知了当前程序可见的 GPU 数量为 4，而我们的 batch size 为 16，输出每个 GPU 上模型 forward 函数内部的 print 内容，验证了每个 GPU 获得的数据量都是 4 个。这表示，DataParallel 会自动帮我们将数据切分、加载到相应 GPU，将模型复制到相应 GPU，进行正向传播计算梯度并汇总。</p><h3 id="多机多卡" tabindex="-1"><a class="header-anchor" href="#多机多卡"><span>多机多卡</span></a></h3><p>多机多卡一般都是基于集群的方式进行大规模的训练，需要涉及非常多的方面，咱们这节课只讨论最基本的原理和方法。在具体实践中，你可能还会遇到其它网络或环境等问题，届时需要具体问题具体解决。</p><h4 id="dp-与-ddp" tabindex="-1"><a class="header-anchor" href="#dp-与-ddp"><span>DP 与 DDP</span></a></h4><p>刚才我们已经提到，对于单机多卡训练，有一个最简单的办法：DataParallel。其实 PyTorch 的数据并行还有一个主要的 API，那就是 DistributedDataParallel。而<strong>DistributedDataParallel 也是我们实现多机多卡的关键 API</strong>。</p><p>DataParallel 简称为 DP，而 DistributedDataParallel 简称为 DDP。我们来详细看看 DP 与 DDP 的区别。</p><p>先看 DP，DP 是单进程控制多 GPU。从之前的程序中，我们也可以看出，DP 将输入的一个 batch 数据分成了 n 份（n 为实际使用的 GPU 数量），分别送到对应的 GPU 进行计算。</p><p>在网络前向传播时，模型会从主 GPU 复制到其它 GPU 上；在反向传播时，每个 GPU 上的梯度汇总到主 GPU 上，求得梯度均值更新模型参数后，再复制到其它 GPU，以此来实现并行。</p><p>由于主 GPU 要进行梯度汇总和模型更新，并将计算任务下发给其它 GPU，所以主 GPU 的负载与使用率会比其它 GPU 高，这就导致了 GPU 负载不均衡的现象。</p><p>再说说 DDP，DDP 多进程控制多 GPU。系统会为每个 GPU 创建一个进程，不再有主 GPU，每个 GPU 执行相同的任务。DDP 使用分布式数据采样器（DistributedSampler）加载数据，确保数据在各个进程之间没有重叠。</p><p>在反向传播时，各 GPU 梯度计算完成后，各进程以广播的方式将梯度进行汇总平均，然后每个进程在各自的 GPU 上进行梯度更新，从而确保每个 GPU 上的模型参数始终保持一致。由于无需在不同 GPU 之间复制模型，DPP 的传输数据量更少，因此速度更快。</p><p><strong>DistributedDataParallel 既可用于单机多卡也可用于多机多卡</strong>，它能够解决 DataParallel 速度慢、GPU 负载不均衡等问题。因此，官方更推荐使用 DistributedDataParallel 来进行分布式训练，也就是接下来要说的 DDP 训练。</p><h4 id="ddp-训练" tabindex="-1"><a class="header-anchor" href="#ddp-训练"><span>DDP 训练</span></a></h4><p>DistributedDataParallel 主要是为多机多卡而设计的，不过单机上也同样可以使用。</p><p>想要弄明白 DPP 的训练机制，我们先要弄明白这几个分布式中的概念：</p><ul><li>group：即进程组。默认情况下，只有一个组，即一个 world。</li><li>world_size ：表示全局进程个数。</li><li>rank：表示进程序号，用于进程间通讯，表示进程优先级。rank=0 的主机为主节点。</li></ul><p>使用 DDP 进行分布式训练的具体流程如下。接下来，我们就按步骤分别去实现。</p><p><img src="https://static001.geekbang.org/resource/image/27/7d/2730a8d7e7e1fe21574918a2dc48c67d.jpg?wh=1920x1009" alt="图片"></p><p>第一步，初始化进程组。我们使用 init_process_group 函数来进行分布式初始化，其定义如下：</p><div class="language-python line-numbers-mode" data-highlighter="prismjs" data-ext="py"><pre><code><span class="line">torch<span class="token punctuation">.</span>distributed<span class="token punctuation">.</span>init_process_group<span class="token punctuation">(</span>backend<span class="token punctuation">,</span> init_method<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span><span class="token punctuation">,</span> world_size<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> rank<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> group_name<span class="token operator">=</span><span class="token string">&#39;&#39;</span><span class="token punctuation">)</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><p>我们分别看看定义里的相关参数：</p><ul><li>backend：是通信所用的后端，可以是“nccl”或“gloo”。一般来说，nccl 用于 GPU 分布式训练，gloo 用于 CPU 进行分布式训练。</li><li>init_method：字符串类型，是一个 url，用于指定进程初始化方式，默认是 “env://”，表示从环境变量初始化，还可以使用 TCP 的方式或共享文件系统  。</li><li>world_size：执行训练的所有的进程数，表示一共有多少个节点（机器）。</li><li>rank：进程的编号，也是其优先级，表示当前节点（机器）的编号。</li><li>group_name：进程组的名字。</li></ul><p>使用 nccl 后端的代码如下。</p><div class="language-python line-numbers-mode" data-highlighter="prismjs" data-ext="py"><pre><code><span class="line">torch<span class="token punctuation">.</span>distributed<span class="token punctuation">.</span>init_process_group<span class="token punctuation">(</span>backend<span class="token operator">=</span><span class="token string">&quot;nccl&quot;</span><span class="token punctuation">)</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><p>完成初始化以后，第二步就是模型并行化。正如前面讲过的，我们可以使用 DistributedDataParallel，将模型分发至多 GPU 上，其定义如下：</p><div class="language-python line-numbers-mode" data-highlighter="prismjs" data-ext="py"><pre><code><span class="line">torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>parallel<span class="token punctuation">.</span>DistributedDataParallel<span class="token punctuation">(</span>module<span class="token punctuation">,</span> device_ids<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> output_device<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">0</span>）</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><p>DistributedDataParallel 的参数与 DataParallel 基本相同，因此模型并行化的用法只需将 DataParallel 函数替换成 DistributedDataParallel 即可，具体代码如下。</p><div class="language-python line-numbers-mode" data-highlighter="prismjs" data-ext="py"><pre><code><span class="line">net <span class="token operator">=</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>parallel<span class="token punctuation">.</span>DistributedDataParallel<span class="token punctuation">(</span>net<span class="token punctuation">)</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><p>最后就是创建分布式数据采样器。在多机多卡情况下，分布式训练数据的读取也是一个问题，不同的卡读取到的数据应该是不同的。</p><p>DP 是直接将一个 batch 的数据划分到不同的卡，但是多机多卡之间进行频繁的数据传输会严重影响效率，这时就要用到分布式数据采样器 DistributedSampler，它会为每个子进程划分出一部分数据集，从而使 DataLoader 只会加载特定的一个子数据集，以避免不同进程之间有数据重复。</p><p>创建与使用分布式数据采样器的代码如下。</p><div class="language-python line-numbers-mode" data-highlighter="prismjs" data-ext="py"><pre><code><span class="line">train_sampler <span class="token operator">=</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data<span class="token punctuation">.</span>distributed<span class="token punctuation">.</span>DistributedSampler<span class="token punctuation">(</span>train_dataset<span class="token punctuation">)</span></span>
<span class="line">data_loader <span class="token operator">=</span> DataLoader<span class="token punctuation">(</span>train_dataset<span class="token punctuation">,</span> batch_size<span class="token operator">=</span>batch_size<span class="token punctuation">,</span> sampler<span class="token operator">=</span>train_sampler<span class="token punctuation">)</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div></div></div><p>结合代码我给你解读一下。 首先，我们将 train_dataset 送到了 DistributedSampler 中，并创建了一个分布式数据采样器 train_sampler。</p><p>然后在构造 DataLoader 的时候,  参数中传入了一个 sampler=train_sampler，即可让不同的进程节点加载属于自己的那份子数据集。也就是说，使用 DDP 时，不再是从主 GPU 分发数据到其他 GPU 上，而是各 GPU 从自己的硬盘上读取属于自己的那份数据。</p><h1 id="" tabindex="-1"><a class="header-anchor" href="#"><span></span></a></h1><p>为什么要使用分布式训练以及分布式训练的原理我们就讲到这里。相信你已经对数据并行与模型并行都有了一个初步的认识。</p><h2 id="小试牛刀" tabindex="-1"><a class="header-anchor" href="#小试牛刀"><span>小试牛刀</span></a></h2><p>下面我们将会讲解一个<a href="https://github.com/pytorch/examples/blob/master/imagenet/main.py" target="_blank" rel="noopener noreferrer">官方的 ImageNet 的示例</a>，以后你可以把这个小项目当做分布式训练的一个模板来使用。</p><p>这个示例可对使用 DP 或 DDP 进行选配，下面我们就一起来看核心代码。</p><div class="language-python line-numbers-mode" data-highlighter="prismjs" data-ext="py"><pre><code><span class="line"><span class="token keyword">if</span> args<span class="token punctuation">.</span>distributed<span class="token punctuation">:</span></span>
<span class="line">     <span class="token keyword">if</span> args<span class="token punctuation">.</span>dist_url <span class="token operator">==</span> <span class="token string">&quot;env://&quot;</span> <span class="token keyword">and</span> args<span class="token punctuation">.</span>rank <span class="token operator">==</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">:</span></span>
<span class="line">         args<span class="token punctuation">.</span>rank <span class="token operator">=</span> <span class="token builtin">int</span><span class="token punctuation">(</span>os<span class="token punctuation">.</span>environ<span class="token punctuation">[</span><span class="token string">&quot;RANK&quot;</span><span class="token punctuation">]</span><span class="token punctuation">)</span></span>
<span class="line">     <span class="token keyword">if</span> args<span class="token punctuation">.</span>multiprocessing_distributed<span class="token punctuation">:</span></span>
<span class="line">         <span class="token comment"># For multiprocessing distributed training, rank needs to be the</span></span>
<span class="line">         <span class="token comment"># global rank among all the processes</span></span>
<span class="line">         args<span class="token punctuation">.</span>rank <span class="token operator">=</span> args<span class="token punctuation">.</span>rank <span class="token operator">*</span> ngpus_per_node <span class="token operator">+</span> gpu</span>
<span class="line">     dist<span class="token punctuation">.</span>init_process_group<span class="token punctuation">(</span>backend<span class="token operator">=</span>args<span class="token punctuation">.</span>dist_backend<span class="token punctuation">,</span> init_method<span class="token operator">=</span>args<span class="token punctuation">.</span>dist_url<span class="token punctuation">,</span></span>
<span class="line">                             world_size<span class="token operator">=</span>args<span class="token punctuation">.</span>world_size<span class="token punctuation">,</span> rank<span class="token operator">=</span>args<span class="token punctuation">.</span>rank<span class="token punctuation">)</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>这里你可以重点关注示例代码中的“args.distributed”参数，args.distributed 为 True，表示使用 DDP，反之表示使用 DP。</p><p>我们来看 main_worker 函数中这段针对 DDP 的初始化代码，如果使用 DDP，那么使用 init_process_group 函数初始化进程组。ngpus_per_node 表示每个节点的 GPU 数量。</p><p>我们再来看 main_worker 函数中的这段逻辑代码。</p><div class="language-python line-numbers-mode" data-highlighter="prismjs" data-ext="py"><pre><code><span class="line"><span class="token keyword">if</span> <span class="token keyword">not</span> torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>is_available<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span></span>
<span class="line">    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&#39;using CPU, this will be slow&#39;</span><span class="token punctuation">)</span></span>
<span class="line"><span class="token keyword">elif</span> args<span class="token punctuation">.</span>distributed<span class="token punctuation">:</span></span>
<span class="line">    <span class="token comment"># For multiprocessing distributed, DistributedDataParallel constructor</span></span>
<span class="line">    <span class="token comment"># should always set the single device scope, otherwise,</span></span>
<span class="line">    <span class="token comment"># DistributedDataParallel will use all available devices.</span></span>
<span class="line">    <span class="token keyword">if</span> args<span class="token punctuation">.</span>gpu <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span></span>
<span class="line">        torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>set_device<span class="token punctuation">(</span>args<span class="token punctuation">.</span>gpu<span class="token punctuation">)</span></span>
<span class="line">        model<span class="token punctuation">.</span>cuda<span class="token punctuation">(</span>args<span class="token punctuation">.</span>gpu<span class="token punctuation">)</span></span>
<span class="line">        <span class="token comment"># When using a single GPU per process and per</span></span>
<span class="line">        <span class="token comment"># DistributedDataParallel, we need to divide the batch size</span></span>
<span class="line">        <span class="token comment"># ourselves based on the total number of GPUs we have</span></span>
<span class="line">        args<span class="token punctuation">.</span>batch_size <span class="token operator">=</span> <span class="token builtin">int</span><span class="token punctuation">(</span>args<span class="token punctuation">.</span>batch_size <span class="token operator">/</span> ngpus_per_node<span class="token punctuation">)</span></span>
<span class="line">        args<span class="token punctuation">.</span>workers <span class="token operator">=</span> <span class="token builtin">int</span><span class="token punctuation">(</span><span class="token punctuation">(</span>args<span class="token punctuation">.</span>workers <span class="token operator">+</span> ngpus_per_node <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">/</span> ngpus_per_node<span class="token punctuation">)</span></span>
<span class="line">        model <span class="token operator">=</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>parallel<span class="token punctuation">.</span>DistributedDataParallel<span class="token punctuation">(</span>model<span class="token punctuation">,</span> device_ids<span class="token operator">=</span><span class="token punctuation">[</span>args<span class="token punctuation">.</span>gpu<span class="token punctuation">]</span><span class="token punctuation">)</span></span>
<span class="line">    <span class="token keyword">else</span><span class="token punctuation">:</span></span>
<span class="line">        model<span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token punctuation">)</span></span>
<span class="line">        <span class="token comment"># DistributedDataParallel will divide and allocate batch_size to all</span></span>
<span class="line">        <span class="token comment"># available GPUs if device_ids are not set</span></span>
<span class="line">        model <span class="token operator">=</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>parallel<span class="token punctuation">.</span>DistributedDataParallel<span class="token punctuation">(</span>model<span class="token punctuation">)</span></span>
<span class="line"><span class="token keyword">elif</span> args<span class="token punctuation">.</span>gpu <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span></span>
<span class="line">    torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>set_device<span class="token punctuation">(</span>args<span class="token punctuation">.</span>gpu<span class="token punctuation">)</span></span>
<span class="line">    model <span class="token operator">=</span> model<span class="token punctuation">.</span>cuda<span class="token punctuation">(</span>args<span class="token punctuation">.</span>gpu<span class="token punctuation">)</span></span>
<span class="line"><span class="token keyword">else</span><span class="token punctuation">:</span></span>
<span class="line">    <span class="token comment"># DataParallel will divide and allocate batch_size to all available GPUs</span></span>
<span class="line">    <span class="token keyword">if</span> args<span class="token punctuation">.</span>arch<span class="token punctuation">.</span>startswith<span class="token punctuation">(</span><span class="token string">&#39;alexnet&#39;</span><span class="token punctuation">)</span> <span class="token keyword">or</span> args<span class="token punctuation">.</span>arch<span class="token punctuation">.</span>startswith<span class="token punctuation">(</span><span class="token string">&#39;vgg&#39;</span><span class="token punctuation">)</span><span class="token punctuation">:</span></span>
<span class="line">        model<span class="token punctuation">.</span>features <span class="token operator">=</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>DataParallel<span class="token punctuation">(</span>model<span class="token punctuation">.</span>features<span class="token punctuation">)</span></span>
<span class="line">        model<span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token punctuation">)</span></span>
<span class="line">    <span class="token keyword">else</span><span class="token punctuation">:</span></span>
<span class="line">        model <span class="token operator">=</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>DataParallel<span class="token punctuation">(</span>model<span class="token punctuation">)</span><span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token punctuation">)</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>这段代码是对使用 CPU 还是使用 GPU、如果使用 GPU，是使用 DP 还是 DDP 进行了逻辑选择。我们可以看到，这里用到了 DistributedDataParallel 函数或 DataParallel 函数，对模型进行并行化。 并行化之后就是创建分布式数据采样器，具体代码如下。</p><div class="language-python line-numbers-mode" data-highlighter="prismjs" data-ext="py"><pre><code><span class="line"><span class="token keyword">if</span> args<span class="token punctuation">.</span>distributed<span class="token punctuation">:</span></span>
<span class="line">    train_sampler <span class="token operator">=</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data<span class="token punctuation">.</span>distributed<span class="token punctuation">.</span>DistributedSampler<span class="token punctuation">(</span>train_dataset<span class="token punctuation">)</span></span>
<span class="line"><span class="token keyword">else</span><span class="token punctuation">:</span></span>
<span class="line">    train_sampler <span class="token operator">=</span> <span class="token boolean">None</span></span>
<span class="line"></span>
<span class="line">train_loader <span class="token operator">=</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data<span class="token punctuation">.</span>DataLoader<span class="token punctuation">(</span></span>
<span class="line">    train_dataset<span class="token punctuation">,</span> batch_size<span class="token operator">=</span>args<span class="token punctuation">.</span>batch_size<span class="token punctuation">,</span> shuffle<span class="token operator">=</span><span class="token punctuation">(</span>train_sampler <span class="token keyword">is</span> <span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">,</span></span>
<span class="line">    num_workers<span class="token operator">=</span>args<span class="token punctuation">.</span>workers<span class="token punctuation">,</span> pin_memory<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> sampler<span class="token operator">=</span>train_sample</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>这里需要注意的是，<strong>在建立 Dataloader 的过程中，如果 sampler 参数不为 None，那么 shuffle 参数不应该被设置</strong>。</p><p>最后，我们需要为每个机器节点上的每个 GPU 启动一个进程。PyTorch 提供了 torch.multiprocessing.spawn 函数，来在一个节点启动该节点所有进程，具体的代码如下。</p><div class="language-python line-numbers-mode" data-highlighter="prismjs" data-ext="py"><pre><code><span class="line"> ngpus_per_node <span class="token operator">=</span> torch<span class="token punctuation">.</span>cuda<span class="token punctuation">.</span>device_count<span class="token punctuation">(</span><span class="token punctuation">)</span></span>
<span class="line"> <span class="token keyword">if</span> args<span class="token punctuation">.</span>multiprocessing_distributed<span class="token punctuation">:</span></span>
<span class="line">     <span class="token comment"># Since we have ngpus_per_node processes per node, the total world_size</span></span>
<span class="line">     <span class="token comment"># needs to be adjusted accordingly</span></span>
<span class="line">     args<span class="token punctuation">.</span>world_size <span class="token operator">=</span> ngpus_per_node <span class="token operator">*</span> args<span class="token punctuation">.</span>world_size</span>
<span class="line">     <span class="token comment"># Use torch.multiprocessing.spawn to launch distributed processes: the</span></span>
<span class="line">     <span class="token comment"># main_worker process function</span></span>
<span class="line">     mp<span class="token punctuation">.</span>spawn<span class="token punctuation">(</span>main_worker<span class="token punctuation">,</span> nprocs<span class="token operator">=</span>ngpus_per_node<span class="token punctuation">,</span> args<span class="token operator">=</span><span class="token punctuation">(</span>ngpus_per_node<span class="token punctuation">,</span> args<span class="token punctuation">)</span><span class="token punctuation">)</span></span>
<span class="line"> <span class="token keyword">else</span><span class="token punctuation">:</span></span>
<span class="line">     <span class="token comment"># Simply call main_worker function</span></span>
<span class="line">     main_worker<span class="token punctuation">(</span>args<span class="token punctuation">.</span>gpu<span class="token punctuation">,</span> ngpus_per_node<span class="token punctuation">,</span> args<span class="token punctuation">)</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>对照代码我们梳理一下其中的要点。之前我们提到的 main_worker 函数，就是每个进程中，需要执行的操作。ngpus_per_node 是每个节点的 GPU 数量（每个节点 GPU 数量相同），如果是多进程，ngpus_per_node * args.world_size 则表示所有的节点中一共有多少个 GPU，即总进程数。 一般情况下，进程 0 是主进程，比如我们会在主进程中保存模型或打印 log 信息。</p><p>当节点数为 1 时，实际上就是单机多卡，所以说 DDP 既可以支持多机多卡，也可以支持单机多卡。</p><p>main_worker 函数的调用方法如下。</p><div class="language-python line-numbers-mode" data-highlighter="prismjs" data-ext="py"><pre><code><span class="line">main_worker<span class="token punctuation">(</span>args<span class="token punctuation">.</span>gpu<span class="token punctuation">,</span> ngpus_per_node<span class="token punctuation">,</span> args<span class="token punctuation">)</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><p>其中，args.gpu 表示当前所使用 GPU 的 id。而通过 mp.spawn 调用之后，会为每个节点上的每个 GPU 都启动一个进程，每个进程运行 main_worker(i, ngpus_per_node, args)，其中 i 是从 0 到 ngpus_per_node-1。 模型保存的代码如下。</p><div class="language-python line-numbers-mode" data-highlighter="prismjs" data-ext="py"><pre><code><span class="line"><span class="token keyword">if</span> <span class="token keyword">not</span> args<span class="token punctuation">.</span>multiprocessing_distributed <span class="token keyword">or</span> <span class="token punctuation">(</span>args<span class="token punctuation">.</span>multiprocessing_distributed</span>
<span class="line">         <span class="token keyword">and</span> args<span class="token punctuation">.</span>rank <span class="token operator">%</span> ngpus_per_node <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">:</span></span>
<span class="line">     save_checkpoint<span class="token punctuation">(</span><span class="token punctuation">{</span></span>
<span class="line">         <span class="token string">&#39;epoch&#39;</span><span class="token punctuation">:</span> epoch <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">,</span></span>
<span class="line">         <span class="token string">&#39;arch&#39;</span><span class="token punctuation">:</span> args<span class="token punctuation">.</span>arch<span class="token punctuation">,</span></span>
<span class="line">         <span class="token string">&#39;state_dict&#39;</span><span class="token punctuation">:</span> model<span class="token punctuation">.</span>state_dict<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span></span>
<span class="line">         <span class="token string">&#39;best_acc1&#39;</span><span class="token punctuation">:</span> best_acc1<span class="token punctuation">,</span></span>
<span class="line">         <span class="token string">&#39;optimizer&#39;</span> <span class="token punctuation">:</span> optimizer<span class="token punctuation">.</span>state_dict<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span></span>
<span class="line">     <span class="token punctuation">}</span><span class="token punctuation">,</span> is_best<span class="token punctuation">)</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>这里需要注意的是，使用 DDP 意味着使用多进程，如果直接保存模型，每个进程都会执行一次保存操作，此时只使用主进程中的一个 GPU 来保存即可。</p><p>好，说到这，这个示例中有关分布式训练的重点内容我们就讲完了。</p><h2 id="小结" tabindex="-1"><a class="header-anchor" href="#小结"><span>小结</span></a></h2><p>恭喜你走到这里，这节课我们一起完成了分布式训练的学习，最后咱们一起做个总结。</p><p>今天我们不但学习了为什么要使用分布式训练以及分布式训练的原理，还一起学习了一个分布式训练的实战项目。</p><p>在分布式训练中，主要有 DP 与 DDP 两种模式。其中 DP 并不是完整的分布式计算，只是将一部分计算放到了多张 GPU 卡上，在计算梯度的时候，仍然是“一卡有难，八方围观”，因此 DP 会有负载不平衡、效率低等问题。而 DDP 刚好能够解决 DP 的上述问题，并且既可以用于单机多卡，也可以用于多机多卡，因此它是更好的分布式训练解决方案。</p><p>你可以将今天讲解的示例当做分布式训练的一个模板来使用。它包括了 DP 与 DPP 的完整使用过程，并且包含了如何在使用 DDP 时保存模型。不过这个示例中的代码里其实还有更多的细节，建议你留用课后空余时间，通过精读代码、查阅资料，多动手、多思考来巩固今天的学习成果。</p><h2 id="每课一练" tabindex="-1"><a class="header-anchor" href="#每课一练"><span>每课一练</span></a></h2><p>在 torch.distributed.init_process_group(backend=&quot;nccl&quot;)函数中，backend 参数可选哪些后端，它们分别有什么区别？</p><p>推荐你好好研读今天的分布式训练 demo，也欢迎你记录自己的学习感悟或疑问，我在留言区等你。</p>`,101)]))}const i=s(t,[["render",l]]),u=JSON.parse('{"path":"/3.tech/83.PyTorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98/16_%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E7%AF%87-%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83%EF%BC%9A%E5%A6%82%E4%BD%95%E5%8A%A0%E9%80%9F%E4%BD%A0%E7%9A%84%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83.html","title":"模型训练篇-分布式训练：如何加速你的模型训练","lang":"zh-cn","frontmatter":{},"headers":[{"level":2,"title":"分布式训练原理","slug":"分布式训练原理","link":"#分布式训练原理","children":[{"level":3,"title":"单机单卡","slug":"单机单卡","link":"#单机单卡","children":[]},{"level":3,"title":"单机多卡","slug":"单机多卡","link":"#单机多卡","children":[]},{"level":3,"title":"多机多卡","slug":"多机多卡","link":"#多机多卡","children":[]}]},{"level":2,"title":"小试牛刀","slug":"小试牛刀","link":"#小试牛刀","children":[]},{"level":2,"title":"小结","slug":"小结","link":"#小结","children":[]},{"level":2,"title":"每课一练","slug":"每课一练","link":"#每课一练","children":[]}],"git":{"updatedTime":1746672966000,"contributors":[{"name":"guoxin-qiu","username":"guoxin-qiu","email":"guoxin.qiu@outlook.com","commits":1,"url":"https://github.com/guoxin-qiu"}],"changelog":[{"hash":"873191059aa4709eddd6184a409223b5054edb2a","time":1746672966000,"email":"guoxin.qiu@outlook.com","author":"guoxin-qiu","message":"update: pytorch fixed"}]},"filePathRelative":"3.tech/83.PyTorch深度学习实战/16_模型训练篇-分布式训练：如何加速你的模型训练.md"}');export{i as comp,u as data};
